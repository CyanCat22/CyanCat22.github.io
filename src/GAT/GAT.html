<!DOCTYPE html>
<html lang="zh-cmn-Hans" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GAT 图神经网络学习笔记 | Gua's Blog</title>
    <meta name="description" content="呱呱的博客, GuaGua blog">
    <link rel="preload stylesheet" href="/assets/style.aec4d03c.css" as="style">
    <script type="module" src="/assets/app.e0ff3824.js"></script>
    <link rel="modulepreload" href="/assets/chunks/framework.11961b2a.js">
    <link rel="modulepreload" href="/assets/chunks/theme.0b6af2c9.js">
    <link rel="modulepreload" href="/assets/src_GAT_GAT.md.15c780bf.lean.js">
    
    <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body><!--v-if--><!--teleport anchor-->
    <div id="app"><div class="Layout" data-v-a71b6ee6 data-v-fab3d334><!--[--><!----><!--[--><div class="theme-blog-popover" style="display:none;" data-pagefind-ignore="all" data-v-f1f6e4fb><div class="header" data-v-f1f6e4fb><div class="title-wrapper" data-v-f1f6e4fb><i class="el-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M288 128h608L736 384l160 256H288v320h-96V64h96z"></path></svg><!--]--></i><span class="title" data-v-f1f6e4fb></span></div><i class="el-icon close-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M512 64a448 448 0 1 1 0 896 448 448 0 0 1 0-896m0 393.664L407.936 353.6a38.4 38.4 0 1 0-54.336 54.336L457.664 512 353.6 616.064a38.4 38.4 0 1 0 54.336 54.336L512 566.336 616.064 670.4a38.4 38.4 0 1 0 54.336-54.336L566.336 512 670.4 407.936a38.4 38.4 0 1 0-54.336-54.336z"></path></svg><!--]--></i></div><!----><div class="footer content" data-v-f1f6e4fb><!--[--><!--]--></div></div><div class="theme-blog-popover-close" style="display:none;" data-v-f1f6e4fb><i class="el-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M288 128h608L736 384l160 256H288v320h-96V64h96z"></path></svg><!--]--></i></div><!--]--><!--]--><!--[--><span tabindex="-1" data-v-151f2593></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-151f2593> Skip to content </a><!--]--><!----><header class="VPNav" data-v-fab3d334 data-v-0fa0e57d><div class="VPNavBar has-sidebar" data-v-0fa0e57d data-v-be450ad9><div class="container" data-v-be450ad9><div class="title" data-v-be450ad9><div class="VPNavBarTitle has-sidebar" data-v-be450ad9 data-v-6d2fb2d9><a class="title" href="/" data-v-6d2fb2d9><!--[--><!--]--><!--[--><img class="VPImage logo" src="https://avatars.githubusercontent.com/u/122679149?v=4" alt data-v-6db2186b><!--]--><!--[-->Gua&#39;s Blog<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-be450ad9><div class="curtain" data-v-be450ad9></div><div class="content-body" data-v-be450ad9><!--[--><!--[--><!--[--><div class="blog-search" data-pagefind-ignore="all" data-v-a71b6ee6 style="--4c3f391a:1;" data-v-3ff534af><div class="nav-search-btn-wait" data-v-3ff534af><svg width="14" height="14" viewBox="0 0 20 20" data-v-3ff534af><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round" data-v-3ff534af></path></svg><span class="search-tip" data-v-3ff534af>搜索</span><span class="metaKey" data-v-3ff534af> K </span></div><!--teleport start--><!--teleport end--></div><!--]--><!--]--><!--]--><!----><!----><!----><div class="VPNavBarAppearance appearance" data-v-be450ad9 data-v-da3f667a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-da3f667a data-v-0d529b6d data-v-f3c41672><span class="check" data-v-f3c41672><span class="icon" data-v-f3c41672><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0d529b6d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0d529b6d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-be450ad9 data-v-2ab2a029 data-v-f6988cfb><!--[--><a class="VPSocialLink" href="https://github.com/CyanCat22" target="_blank" rel="noopener" data-v-f6988cfb data-v-e57698f6><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-be450ad9 data-v-66bb1f24 data-v-96001b6b><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-96001b6b><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-96001b6b><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-96001b6b><div class="VPMenu" data-v-96001b6b data-v-e7ea1737><!----><!--[--><!--[--><!----><div class="group" data-v-66bb1f24><div class="item appearance" data-v-66bb1f24><p class="label" data-v-66bb1f24>Appearance</p><div class="appearance-action" data-v-66bb1f24><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-66bb1f24 data-v-0d529b6d data-v-f3c41672><span class="check" data-v-f3c41672><span class="icon" data-v-f3c41672><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0d529b6d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0d529b6d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-66bb1f24><div class="item social-links" data-v-66bb1f24><div class="VPSocialLinks social-links-list" data-v-66bb1f24 data-v-f6988cfb><!--[--><a class="VPSocialLink" href="https://github.com/CyanCat22" target="_blank" rel="noopener" data-v-f6988cfb data-v-e57698f6><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-be450ad9 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav" data-v-fab3d334 data-v-2817d72e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2817d72e><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-2817d72e><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-2817d72e>Menu</span></button><a class="top-link" href="#" data-v-2817d72e>Return to top</a></div><aside class="VPSidebar" data-v-fab3d334 data-v-c79ccefa><div class="curtain" data-v-c79ccefa></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-c79ccefa><span class="visually-hidden" id="sidebar-aria-label" data-v-c79ccefa> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-c79ccefa><div class="VPSidebarItem level-0" data-v-c79ccefa data-v-983f6b21><!----><!----></div></div><!--]--><!--[--><!--[--><div class="sidebar" data-pagefind-ignore="all" data-v-a71b6ee6 data-v-e42837f7><div class="card recommend" data-pagefind-ignore="all" data-v-e42837f7 data-v-2844b86b><div class="card-header" data-v-2844b86b><span class="title" data-v-2844b86b>🔍 相关文章</span><!----></div><div class="empty-text" data-v-2844b86b>暂无推荐文章</div></div></div><!--]--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-pagefind-body data-v-fab3d334 data-v-fe5dac9b><div class="VPDoc has-sidebar has-aside" data-v-fe5dac9b data-v-ae7d01fc><div class="container" data-v-ae7d01fc><div class="aside" data-v-ae7d01fc><div class="aside-curtain" data-v-ae7d01fc></div><div class="aside-container" data-v-ae7d01fc><div class="aside-content" data-v-ae7d01fc><div class="VPDocAside" data-v-ae7d01fc data-v-cdc66372><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-cdc66372 data-v-6106c300><div class="content" data-v-6106c300><div class="outline-marker" data-v-6106c300></div><div class="outline-title" data-v-6106c300>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-6106c300><span class="visually-hidden" id="doc-outline-aria-label" data-v-6106c300> Table of Contents for current page </span><ul class="root" data-v-6106c300 data-v-1188541a><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-cdc66372></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-ae7d01fc><div class="content-container" data-v-ae7d01fc><!--[--><!--[--><!--[--><!----><!----><!--]--><!--]--><!--]--><main class="main" data-v-ae7d01fc><div style="position:relative;" class="vp-doc _src_GAT_GAT" data-v-ae7d01fc><div><h1 id="gat-图神经网络学习笔记" tabindex="-1">GAT 图神经网络学习笔记 <a class="header-anchor" href="#gat-图神经网络学习笔记" aria-hidden="true">#</a></h1><p><em>关于图、GAT 的学习记录</em></p><p><em>为何叫 GAT 捏，因为 GAN 一般指的是 Generative Adversal Nets</em></p><h2 id="数据结构-图" tabindex="-1">数据结构-图 <a class="header-anchor" href="#数据结构-图" aria-hidden="true">#</a></h2><p>图的三个特征</p><ul><li><code>node</code> 节点，每个顶点有着自己的特征，node2vec 将顶点的特征转换为一个高维向量 <!----></li><li><code>edge</code> 边，节点之间的连接</li><li><code>global</code> 图，包含着全局特征</li></ul><p>下面这个图就是一个无向连接的图 <!----> ，其有五个节点，每个节点有<strong>相邻节点</strong>和此节点自身的<strong>特征（可以是一个数值、向量、矩阵）</strong><br><img src="https://pic1.zhimg.com/80/v2-ec415ca61d7eef27296aff1994e91db8_1440w.webp" alt=""></p><p>可写出图 G 的邻接矩阵 A ：</p><p><img src="/assets/邻接矩阵.dfe02fca.png" alt=""></p><h2 id="inductive-和-transductive" tabindex="-1">inductive 和 transductive <a class="header-anchor" href="#inductive-和-transductive" aria-hidden="true">#</a></h2><p><a href="https://www.zhihu.com/question/68275921" target="_blank" rel="noreferrer">如何理解 inductive learning 与 transductive learning?</a><br> 区别在于<strong>预测的样本</strong>是否在我们训练的时候已经用过<br><code>transductive</code> 直推式学习 训练阶段和测试阶段都基于同样的图结构<br><code>inductive</code> 归纳式学习 处理动态图 训练阶段和测试阶段要处理的图不同</p><hr><h2 id="gat-架构" tabindex="-1">GAT 架构 <a class="header-anchor" href="#gat-架构" aria-hidden="true">#</a></h2><p><strong>Graph Attention Network</strong></p><p>有着两种计算方式<br><code>Global Graph Attention</code> 节点 i 与图上所有节点都做 attention 计算<br><code>Mask Graph Attention</code> 节点 i 只与邻居节点做 attention 计算， GAT 使用的是这个方法</p><h3 id="graph-attentional-layer" tabindex="-1">Graph Attentional Layer <a class="header-anchor" href="#graph-attentional-layer" aria-hidden="true">#</a></h3><p>GAT 架构通过堆叠图注意力层来实现<br> 先来看注意力系数(attention coefficients)的计算：</p><!----><p>eij 表示节点 j 的特征对节点 i 的影响</p><p>下面为 GAL 的核心公式：</p><!----><p><em>其中 || 为 concatenate，表示张量的粘合，比如[[1， 2], [3，4]]粘合[[5, 6], [7, 8]]变成[[1， 2], [3，4]，[5, 6], [7, 8]]</em></p><ul><li><!---->和<!---->表示张量的节点 i 和 j 的节点特征，维度是 1 x <!----></li><li><!---->是权重矩阵，维度<!----> x <!----> ，这个权重矩阵是共享的，可以应用于每一个节点</li><li><!----> 为 attention kernal, 维度为<!----> x 1</li><li><!----> 激活函数用的是 LeakyReLu(负斜率=0.2)<br> 关于激活函数可以阅读<a href="https://zhuanlan.zhihu.com/p/172254089" target="_blank" rel="noreferrer">激活函数(Sigmoid/ReLU/LeakyReLU/PReLU/ELU)</a>这篇文章</li></ul><p>观察这些维度，会惊奇的发现经过公式的一顿操作之后的结果是一个实数 R, 这个数就是 attention 系数，表明 j 节点对 i 节点的重要程度</p><p>这时候再看论文里的这个图系不系了然于心 😉</p><p><img src="/assets/图注意力机制.7ad1835b.png" alt=""></p><p>了解了<!---->的计算公式，再来看这个公式</p><!----><ul><li><!----> 表示这层 GAL 关于节点 i 的输出特征</li><li><!----> 表示节点 i 的邻接节点</li><li><!----> 表示上述公式的结果即注意力系数</li><li>这里的<!---->激活函数采用的是 ELU</li></ul><h3 id="multi-head-attention" tabindex="-1">multi-head attention <a class="header-anchor" href="#multi-head-attention" aria-hidden="true">#</a></h3><p>先看这个图</p><p><img src="/assets/多头注意力.58a2fa69.png" alt=""></p><p>图上 3 条不同颜色的线就代表着 3 个独立的 attention 系数</p><!----><p>上面公式代表中间层的输出形式 最后一层即预测层采取加权平方法，<br> 下面的公式为输出层形式：</p><!----><p>输出层的<!---->用的是 softmax</p><h2 id="代码实现" tabindex="-1">代码实现 <a class="header-anchor" href="#代码实现" aria-hidden="true">#</a></h2><p><em>用的是 keras</em></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> head </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">attn_heads</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    kernel </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">kernels</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">head</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># W in the paper (F x F&#39;)</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Attention kernel a in the paper (2F&#39; x 1)</span></span>
<span class="line"><span style="color:#BABED8;">    attention_kernel </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">attn_kernels</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">head</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Compute inputs to attention network</span></span>
<span class="line"><span style="color:#BABED8;">    features </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dot</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">X</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> kernel</span><span style="color:#89DDFF;">)</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># (N x F&#39;)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Compute feature combinations</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># (N x 1), [a_1]^T [Wh_i]</span></span>
<span class="line"><span style="color:#BABED8;">    attn_for_self </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dot</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">features</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> attention_kernel</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># (N x 1), [a_2]^T [Wh_j]</span></span>
<span class="line"><span style="color:#BABED8;">    attn_for_neighs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dot</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">features</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> attention_kernel</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># (N x N) via broadcasting</span></span>
<span class="line"><span style="color:#BABED8;">    dense </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> attn_for_self </span><span style="color:#89DDFF;">+</span><span style="color:#BABED8;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn_for_neighs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Add nonlinearty</span></span>
<span class="line"><span style="color:#BABED8;">    dense </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">LeakyReLU</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">alpha</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0.2</span><span style="color:#89DDFF;">)(</span><span style="color:#82AAFF;">dense</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Mask values before activation (Vaswani et al., 2017)</span></span>
<span class="line"><span style="color:#BABED8;">    mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">10e9</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">*</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1.0</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">-</span><span style="color:#BABED8;"> A</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    dense </span><span style="color:#89DDFF;">+=</span><span style="color:#BABED8;"> mask</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># Apply softmax to get attention coefficients</span></span>
<span class="line"><span style="color:#BABED8;">    dense </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">softmax</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dense</span><span style="color:#89DDFF;">)</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># (N x N)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><h2 id="gat-的优点" tabindex="-1">GAT 的优点 <a class="header-anchor" href="#gat-的优点" aria-hidden="true">#</a></h2><ol><li>计算速度快， 可以在不同节点上进行并行运算</li><li>可以同时对拥有不同度的节点进行处理</li><li>可以被直接用于解决归纳学习(inductive)问题，即可以对从未见过的图结构进行处理</li></ol><hr><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-hidden="true">#</a></h2><p><a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noreferrer">GRAPH ATTENTION NETWORKS</a><br><a href="https://zhuanlan.zhihu.com/p/112938037" target="_blank" rel="noreferrer">【GNN】图注意力网络 GAT</a><br><a href="https://github.com/danielegrattarola/keras-gat" target="_blank" rel="noreferrer">keras 实现 GAT</a><br><a href="https://github.com/Diego999/pyGAT" target="_blank" rel="noreferrer">pytorch 实现 GAT</a></p><h2 id="英语学习" tabindex="-1">英语学习 <a class="header-anchor" href="#英语学习" aria-hidden="true">#</a></h2><h3 id="专业类词汇" tabindex="-1">专业类词汇 <a class="header-anchor" href="#专业类词汇" aria-hidden="true">#</a></h3><p>inductive 归纳<br> transductive 转导<br> benchmark 基准<br> recursive 递归的、循环的<br> grid-like 网格状<br> acyclic 非周期、非环状<br> cyclic 有环的<br> aggregated 聚集<br> explicit 显式的<br> coefficient 系数</p><h3 id="描述类" tabindex="-1">描述类 <a class="header-anchor" href="#描述类" aria-hidden="true">#</a></h3><p>leverage 使用、利用<br> arbitrarily 任意的<br> intense computation 密集计算</p></div></div></main><!--[--><!--]--><footer class="VPDocFooter" data-v-ae7d01fc data-v-2813752b><div class="edit-info" data-v-2813752b><!----><div class="last-updated" data-v-2813752b><p class="VPLastUpdated" data-v-2813752b data-v-355aa5ef>上次更新于: <time datetime="2023-12-12T04:01:00.000Z" data-v-355aa5ef></time></p></div></div><!----></footer><!--[--><!--[--><!--[--><div class="comment" id="giscus-comment" data-pagefind-ignore="all" data-v-a71b6ee6 data-v-8ec3e109><div class="el-affix comment-btn" style="height:;width:;" data-v-8ec3e109><div class="" style=""><!--[--><button ariadisabled="false" type="button" class="el-button el-button--primary is-plain" style="" data-v-8ec3e109><i class="el-icon" style=""><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path fill="currentColor" d="M736 504a56 56 0 1 1 0-112 56 56 0 0 1 0 112m-224 0a56 56 0 1 1 0-112 56 56 0 0 1 0 112m-224 0a56 56 0 1 1 0-112 56 56 0 0 1 0 112M128 128v640h192v160l224-160h352V128z"></path></svg><!--]--></i><span class=""><!--[-->评论<!--]--></span></button><!--]--></div></div><!----></div><!--]--><!--]--><!--]--></div></div></div></div></div><footer class="VPFooter has-sidebar" data-v-fab3d334 data-v-d24360a6><div class="container" data-v-d24360a6><!----><p class="copyright" data-v-d24360a6>不知江月待何人🌱</p></div></footer><!--[--><!--]--></div></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"index.md\":\"4e77a008\",\"src_bert_bert.md\":\"59fd7f77\",\"src_transformer_01_trans_learn_note.md\":\"767e6bd7\",\"about.md\":\"e3505837\",\"src_gat_gat.md\":\"15c780bf\"}")
__VP_SITE_DATA__ = JSON.parse("{\"lang\":\"zh-cmn-Hans\",\"dir\":\"ltr\",\"title\":\"Gua's Blog\",\"description\":\"呱呱的博客, GuaGua blog\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"blog\":{\"pagesData\":[{\"route\":\"/about\",\"meta\":{\"hidden\":true,\"title\":\"关于我:rose:\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"你好！  \\n窝系:star:呱呱:star:  \\n一个快乐阳光滴 girl:stuck_out_tongue_closed_eyes:  \\n我会在这里抒写呱呱的编程之旅，希望能帮助到你~  \\n联系我:\",\"cover\":\"\"}},{\"route\":\"/src/Bert/Bert\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"B\",\"7\":\"e\",\"8\":\"r\",\"9\":\"t\",\"10\":\"_\",\"11\":\"l\",\"12\":\"e\",\"13\":\"a\",\"14\":\"r\",\"15\":\"n\",\"16\":\"_\",\"17\":\"n\",\"18\":\"o\",\"19\":\"t\",\"20\":\"e\",\"21\":\"1\",\"22\":\" \",\"23\":\"t\",\"24\":\"a\",\"25\":\"g\",\"26\":\"s\",\"27\":\":\",\"28\":\"[\",\"29\":\"B\",\"30\":\"e\",\"31\":\"r\",\"32\":\"t\",\"33\":\",\",\"34\":\" \",\"35\":\"n\",\"36\":\"l\",\"37\":\"p\",\"38\":\"]\",\"title\":\"Bert学习笔记🌟\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"全称：Bidirectinal Encoder Representation from Transformer  \\n是 Google 以无监督的方式利用大量无标注的文本练成的语言模型\\nGoogle 在\",\"cover\":\"\"}},{\"route\":\"/src/GAT/GAT\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"G\",\"7\":\"A\",\"8\":\"T\",\"9\":\" \",\"10\":\"学\",\"11\":\"习\",\"12\":\"笔\",\"13\":\"记\",\"14\":\" \",\"15\":\"t\",\"16\":\"a\",\"17\":\"g\",\"18\":\"s\",\"19\":\":\",\"20\":\"[\",\"21\":\"G\",\"22\":\"A\",\"23\":\"T\",\"24\":\",\",\"25\":\" \",\"26\":\"n\",\"27\":\"l\",\"28\":\"p\",\"29\":\"]\",\"title\":\"GAT图神经网络学习笔记\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"_关于图、GAT 的学习记录_\\n_为何叫 GAT 捏，因为 GAN 一般指的是 Generative Adversal Nets_\\n 数据结构-图\\n图的三个特征\\n- `node` 节点，每个顶点有着自\",\"cover\":\"https://pic1.zhimg.com/80/v2-ec415ca61d7eef27296aff1994e91db8_1440w.webp\"}},{\"route\":\"/src/Transformer/01/trans_learn_note\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"T\",\"7\":\"r\",\"8\":\"a\",\"9\":\"n\",\"10\":\"s\",\"11\":\"f\",\"12\":\"o\",\"13\":\"m\",\"14\":\"e\",\"15\":\"r\",\"16\":\"_\",\"17\":\"n\",\"18\":\"o\",\"19\":\"t\",\"20\":\"e\",\"21\":\" \",\"22\":\"t\",\"23\":\"a\",\"24\":\"g\",\"25\":\"s\",\"26\":\":\",\"27\":\"[\",\"28\":\"T\",\"29\":\"r\",\"30\":\"a\",\"31\":\"n\",\"32\":\"s\",\"33\":\"f\",\"34\":\"o\",\"35\":\"r\",\"36\":\"m\",\"37\":\"e\",\"38\":\"r\",\"39\":\",\",\"40\":\"n\",\"41\":\"l\",\"42\":\"p\",\"43\":\"]\",\"title\":\"🌟Transformer学习，实现🌟\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"本文是初学 transformer 的笔记记录、代码实现  \\n还有 pytorch 库中一些函数的用法 tips :\\ntransformer 框架图:point_down:\\n 代码实现\\n 库&参数\\n`\",\"cover\":\"\"}}],\"author\":\"GuaGua\",\"friend\":[{\"nickname\":\"GuaGua's Blog\",\"des\":\"呼噜噜～(￣▽￣～)~\",\"avatar\":\"/avatar.jpg\",\"url\":\"/about\"}],\"comment\":{\"repo\":\"CyanCat22/CyanCat22.github.io\",\"repoId\":\"R_kgDOJONNeg\",\"category\":\"Ideas\",\"categoryId\":\"DIC_kwDOJONNes4CVJhx\",\"mapping\":\"title\",\"loading\":\"lazy\",\"inputPosition\":\"top\"},\"search\":\"pagefind\"},\"sidebar\":[{\"text\":\"\",\"items\":[]}],\"lastUpdatedText\":\"上次更新于\",\"footer\":{\"copyright\":\"不知江月待何人🌱\"},\"logo\":\"https://avatars.githubusercontent.com/u/122679149?v=4\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/CyanCat22\"}]},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}")</script>
    
  </body>
</html>