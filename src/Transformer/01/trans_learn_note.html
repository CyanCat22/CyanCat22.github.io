<!DOCTYPE html>
<html lang="zh-cmn-Hans" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>🌟Transformer 学习，实现 🌟 | Gua's Blog</title>
    <meta name="description" content="呱呱的博客, GuaGua blog">
    <link rel="preload stylesheet" href="/assets/style.aec4d03c.css" as="style">
    <script type="module" src="/assets/app.e0ff3824.js"></script>
    <link rel="modulepreload" href="/assets/chunks/framework.11961b2a.js">
    <link rel="modulepreload" href="/assets/chunks/theme.0b6af2c9.js">
    <link rel="modulepreload" href="/assets/src_Transformer_01_trans_learn_note.md.767e6bd7.lean.js">
    
    <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body><!--v-if--><!--teleport anchor-->
    <div id="app"><div class="Layout" data-v-a71b6ee6 data-v-fab3d334><!--[--><!----><!--[--><div class="theme-blog-popover" style="display:none;" data-pagefind-ignore="all" data-v-f1f6e4fb><div class="header" data-v-f1f6e4fb><div class="title-wrapper" data-v-f1f6e4fb><i class="el-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M288 128h608L736 384l160 256H288v320h-96V64h96z"></path></svg><!--]--></i><span class="title" data-v-f1f6e4fb></span></div><i class="el-icon close-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M512 64a448 448 0 1 1 0 896 448 448 0 0 1 0-896m0 393.664L407.936 353.6a38.4 38.4 0 1 0-54.336 54.336L457.664 512 353.6 616.064a38.4 38.4 0 1 0 54.336 54.336L512 566.336 616.064 670.4a38.4 38.4 0 1 0 54.336-54.336L566.336 512 670.4 407.936a38.4 38.4 0 1 0-54.336-54.336z"></path></svg><!--]--></i></div><!----><div class="footer content" data-v-f1f6e4fb><!--[--><!--]--></div></div><div class="theme-blog-popover-close" style="display:none;" data-v-f1f6e4fb><i class="el-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M288 128h608L736 384l160 256H288v320h-96V64h96z"></path></svg><!--]--></i></div><!--]--><!--]--><!--[--><span tabindex="-1" data-v-151f2593></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-151f2593> Skip to content </a><!--]--><!----><header class="VPNav" data-v-fab3d334 data-v-0fa0e57d><div class="VPNavBar has-sidebar" data-v-0fa0e57d data-v-be450ad9><div class="container" data-v-be450ad9><div class="title" data-v-be450ad9><div class="VPNavBarTitle has-sidebar" data-v-be450ad9 data-v-6d2fb2d9><a class="title" href="/" data-v-6d2fb2d9><!--[--><!--]--><!--[--><img class="VPImage logo" src="https://avatars.githubusercontent.com/u/122679149?v=4" alt data-v-6db2186b><!--]--><!--[-->Gua&#39;s Blog<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-be450ad9><div class="curtain" data-v-be450ad9></div><div class="content-body" data-v-be450ad9><!--[--><!--[--><!--[--><div class="blog-search" data-pagefind-ignore="all" data-v-a71b6ee6 style="--4c3f391a:1;" data-v-3ff534af><div class="nav-search-btn-wait" data-v-3ff534af><svg width="14" height="14" viewBox="0 0 20 20" data-v-3ff534af><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round" data-v-3ff534af></path></svg><span class="search-tip" data-v-3ff534af>搜索</span><span class="metaKey" data-v-3ff534af> K </span></div><!--teleport start--><!--teleport end--></div><!--]--><!--]--><!--]--><!----><!----><!----><div class="VPNavBarAppearance appearance" data-v-be450ad9 data-v-da3f667a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-da3f667a data-v-0d529b6d data-v-f3c41672><span class="check" data-v-f3c41672><span class="icon" data-v-f3c41672><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0d529b6d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0d529b6d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-be450ad9 data-v-2ab2a029 data-v-f6988cfb><!--[--><a class="VPSocialLink" href="https://github.com/CyanCat22" target="_blank" rel="noopener" data-v-f6988cfb data-v-e57698f6><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-be450ad9 data-v-66bb1f24 data-v-96001b6b><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-96001b6b><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-96001b6b><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-96001b6b><div class="VPMenu" data-v-96001b6b data-v-e7ea1737><!----><!--[--><!--[--><!----><div class="group" data-v-66bb1f24><div class="item appearance" data-v-66bb1f24><p class="label" data-v-66bb1f24>Appearance</p><div class="appearance-action" data-v-66bb1f24><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-66bb1f24 data-v-0d529b6d data-v-f3c41672><span class="check" data-v-f3c41672><span class="icon" data-v-f3c41672><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0d529b6d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0d529b6d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-66bb1f24><div class="item social-links" data-v-66bb1f24><div class="VPSocialLinks social-links-list" data-v-66bb1f24 data-v-f6988cfb><!--[--><a class="VPSocialLink" href="https://github.com/CyanCat22" target="_blank" rel="noopener" data-v-f6988cfb data-v-e57698f6><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-be450ad9 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav" data-v-fab3d334 data-v-2817d72e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2817d72e><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-2817d72e><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-2817d72e>Menu</span></button><a class="top-link" href="#" data-v-2817d72e>Return to top</a></div><aside class="VPSidebar" data-v-fab3d334 data-v-c79ccefa><div class="curtain" data-v-c79ccefa></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-c79ccefa><span class="visually-hidden" id="sidebar-aria-label" data-v-c79ccefa> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-c79ccefa><div class="VPSidebarItem level-0" data-v-c79ccefa data-v-983f6b21><!----><!----></div></div><!--]--><!--[--><!--[--><div class="sidebar" data-pagefind-ignore="all" data-v-a71b6ee6 data-v-e42837f7><div class="card recommend" data-pagefind-ignore="all" data-v-e42837f7 data-v-2844b86b><div class="card-header" data-v-2844b86b><span class="title" data-v-2844b86b>🔍 相关文章</span><!----></div><div class="empty-text" data-v-2844b86b>暂无推荐文章</div></div></div><!--]--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-pagefind-body data-v-fab3d334 data-v-fe5dac9b><div class="VPDoc has-sidebar has-aside" data-v-fe5dac9b data-v-ae7d01fc><div class="container" data-v-ae7d01fc><div class="aside" data-v-ae7d01fc><div class="aside-curtain" data-v-ae7d01fc></div><div class="aside-container" data-v-ae7d01fc><div class="aside-content" data-v-ae7d01fc><div class="VPDocAside" data-v-ae7d01fc data-v-cdc66372><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-cdc66372 data-v-6106c300><div class="content" data-v-6106c300><div class="outline-marker" data-v-6106c300></div><div class="outline-title" data-v-6106c300>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-6106c300><span class="visually-hidden" id="doc-outline-aria-label" data-v-6106c300> Table of Contents for current page </span><ul class="root" data-v-6106c300 data-v-1188541a><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-cdc66372></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-ae7d01fc><div class="content-container" data-v-ae7d01fc><!--[--><!--[--><!--[--><!----><!----><!--]--><!--]--><!--]--><main class="main" data-v-ae7d01fc><div style="position:relative;" class="vp-doc _src_Transformer_01_trans_learn_note" data-v-ae7d01fc><div><h1 id="🌟transformer-学习-实现-🌟" tabindex="-1">🌟Transformer 学习，实现 🌟 <a class="header-anchor" href="#🌟transformer-学习-实现-🌟" aria-hidden="true">#</a></h1><p>本文是初学 transformer 的笔记记录、代码实现<br> 还有 pytorch 库中一些函数的用法 tips :&gt;</p><p><strong>transformer 框架图👇</strong><img src="/assets/figure1.97fb2b71.png" alt=""></p><h3 id="代码实现" tabindex="-1">代码实现 <a class="header-anchor" href="#代码实现" aria-hidden="true">#</a></h3><h4 id="库-参数" tabindex="-1">库&amp;参数 <a class="header-anchor" href="#库-参数" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">utils</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">data</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#BABED8;"> Data</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#BABED8;"> nn</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> math</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> numpy </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#BABED8;"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 参数</span></span>
<span class="line"><span style="color:#BABED8;">d_model </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">512</span></span>
<span class="line"><span style="color:#BABED8;">d_ff </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">2048</span></span>
<span class="line"><span style="color:#BABED8;">d_k </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> d_v </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">64</span></span>
<span class="line"><span style="color:#BABED8;">n_layers </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">6</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># number of encoder and decoder layers</span></span>
<span class="line"><span style="color:#BABED8;">n_heads </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">8</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><h4 id="step1-positional-encoding" tabindex="-1">Step1 Positional Encoding <a class="header-anchor" href="#step1-positional-encoding" aria-hidden="true">#</a></h4><p>由于 Transformer 模型没有循环神经网络的迭代操作，所以我们需要提供每个字的<strong>位置信息</strong>给 Transformer，这样它才能识别出语言中的顺序关系<br> 在 transformer 模型里不训练，在 Bert 模型里会进行训练💥 <a href="https://wmathor.com/index.php/archives/1453/" target="_blank" rel="noreferrer">Positional Encoding 文章理解</a></p><ul><li>🔥编码唯一</li><li>🔥值有界</li><li>🔥不同长度的句子之间，任何两个字之间的差值应该一致<br><img src="/assets/position2.1b12b726.png" alt=""><br><img src="/assets/position.c45c9c7f.png" alt=""></li></ul><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">PositionalEncoding</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">位置编码</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">d_module</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dropout</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0.1</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">max_len</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">5000</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PositionalEncoding</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">dropout</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Dropout</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">p</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">dropout</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        pe </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">max_len</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_module</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># unsqueeze =&gt; 在指定位置插入一个新的维度</span></span>
<span class="line"><span style="color:#BABED8;">        position </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> max_len</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">dtype</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">float32</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># (seq_len, batch_size, d_model)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 比例因子</span></span>
<span class="line"><span style="color:#BABED8;">        div_term </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">exp</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_module</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">float</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">(-</span><span style="color:#82AAFF;">math</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">log</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10000.0</span><span style="color:#89DDFF;">)</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> d_module</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 0::2 切片表达式，索引从零开始，步长为2（隔一个索引取一个）</span></span>
<span class="line"><span style="color:#BABED8;">        pe</span><span style="color:#89DDFF;">[:,</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">::</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">position </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> div_term</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        pe</span><span style="color:#89DDFF;">[:,</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">::</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cos</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">position </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> div_term</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        pe </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> pe</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 将位置编码矩阵注册为一个缓冲区, 避免在每次前向传播时重新计算位置编码</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">register_buffer</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">pe</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> pe</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        x </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pe</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">:</span><span style="color:#F07178;"> x</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">:]</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dropout</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><h4 id="tip1-torch-unsqueeze" tabindex="-1">Tip1 <a href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html" target="_blank" rel="noreferrer">torch.unsqueeze()</a> <a class="header-anchor" href="#tip1-torch-unsqueeze" aria-hidden="true">#</a></h4><p>=&gt; 在指定位置插入一个维度 扩张维度<br><strong>torch.squeeze()</strong> =&gt; 删除张量中大小为 1 的维度 代码示例👇</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a1 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a2 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a3 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">squeeze</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a4 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h4 id="tip2-torch-transpose-dim0-dim1" tabindex="-1">Tip2 <a href="https://pytorch.org/docs/stable/generated/torch.transpose.html" target="_blank" rel="noreferrer">torch.transpose(dim0, dim1)</a> <a class="header-anchor" href="#tip2-torch-transpose-dim0-dim1" aria-hidden="true">#</a></h4><p>=&gt; 交换两个指定的维度</p><h4 id="tip3-register-buffer" tabindex="-1">Tip3 register_buffer <a class="header-anchor" href="#tip3-register-buffer" aria-hidden="true">#</a></h4><h4 id="step2-pad-mask-and-subsequence-mask" tabindex="-1">Step2 Pad_Mask and Subsequence Mask <a class="header-anchor" href="#step2-pad-mask-and-subsequence-mask" aria-hidden="true">#</a></h4><p>按照 mini-batch 中最大的句长对剩余的句子进行补齐，一般用 0 进行填充(padding)<br> mask 操作，让无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置</p><p><img src="/assets/padding_mask.05b4f1ae.png" alt=""></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">seq_q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">seq_k</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> len_q </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> seq_q</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">seq_q</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">())</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">seq_k</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#BABED8;">    batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> len_k </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> seq_k</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># data.eq(0) 是比较操作，找出序列中所有等于零的元素,返回一个True（即填充（PAD）token），False 表示其他非填充元素</span></span>
<span class="line"><span style="color:#BABED8;">    pad_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> seq_k</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">data</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">eq</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># 根据指定的形状参数沿着指定的维度扩展输入张量</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># print(pad_attn_mask.expand(batch_size, len_q, len_k))</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> pad_attn_mask</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">expand</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> len_q</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> len_k</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h4 id="tip4-torch-eq" tabindex="-1">Tip4 <a href="https://pytorch.org/docs/stable/generated/torch.eq.html" target="_blank" rel="noreferrer">torch.eq()</a> <a class="header-anchor" href="#tip4-torch-eq" aria-hidden="true">#</a></h4><p>-&gt; A boolean tensor that is True where input is equal to other and False elsewhere<br> eg👇:</p><blockquote><p>torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))<br> -&gt; tensor([[[True, False], [False, True]]])</p></blockquote><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_subsequence_mask</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">seq</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># 在decoder中用到，屏蔽未来时刻的信息</span></span>
<span class="line"><span style="color:#BABED8;">    attn_shape </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[</span><span style="color:#BABED8;">seq</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">),</span><span style="color:#BABED8;"> seq</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#BABED8;"> seq</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)]</span></span>
<span class="line"><span style="color:#BABED8;">    subsequence_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> np</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">triu</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">np</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ones</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn_shape</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">k</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    subsequence_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_numpy</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">subsequence_mask</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">byte</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># torch.from_numpy().byte() 将numpy数组转换为Tensor</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> subsequence_mask</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h4 id="tip5-np-triu" tabindex="-1">Tip5 <a href="https://numpy.org/doc/stable/reference/generated/numpy.triu.html" target="_blank" rel="noreferrer">np.triu()</a> <a class="header-anchor" href="#tip5-np-triu" aria-hidden="true">#</a></h4><ul><li><strong>np.triu(a, k)</strong> 是取矩阵 a 的上三角数据，但这个三角的斜线位置由 k 的值确定。</li><li><strong>np.tril(a, k)</strong> 是取矩阵 a 的下三角数据</li></ul><h4 id="step3-scaleddotproductattention" tabindex="-1">Step3 ScaledDotProductAttention <a class="header-anchor" href="#step3-scaleddotproductattention" aria-hidden="true">#</a></h4><p>点积注意力公式👇 <img src="/assets/attention.d0d7183a.png" alt=""><br><img src="/assets/self_attention.f815085a.png" alt=""></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">ScaledDotProductAttention</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">缩放点积注意力 单词间的权重计算</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ScaledDotProductAttention</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        Q: [batch_size, n_heads, len_q, d_k]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        K: [batch_size, n_heads, len_k, d_k]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">Q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">K</span><span style="color:#89DDFF;">:</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Tensor</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">V</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 将Q和K的最后一个维度进行点积，在最后一个维度上进行的。</span></span>
<span class="line"><span style="color:#BABED8;">        scores</span><span style="color:#89DDFF;">:</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Tensor</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            Q</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">))</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">/</span><span style="color:#BABED8;"> np</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sqrt</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_k</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># mask --- qt~qn =&gt; 很大的负数</span></span>
<span class="line"><span style="color:#BABED8;">        scores</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">masked_fill_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn_mask</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1e9</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># softmax()高得分接近1，低得分接近0，所有概率之和为1</span></span>
<span class="line"><span style="color:#BABED8;">        attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Softmax</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">dim</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)(</span><span style="color:#82AAFF;">scores</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 再乘值向量得到上下文的权重</span></span>
<span class="line"><span style="color:#BABED8;">        context </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> V</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> context</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><h4 id="step4-multiheadattention" tabindex="-1">Step4 MultiHeadAttention <a class="header-anchor" href="#step4-multiheadattention" aria-hidden="true">#</a></h4><p>定义多组，让它们分别关注不同的上下文 也增加了可学习的参数 W_Q, W_K, W_V</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">MultiHeadAttention</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">W_Q</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">W_K</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">W_V</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_v </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 将多头注意力的输出进行聚合和转换，将输入维度（batch_size,n_heads*d_v)转换为(~, d_model)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">fc</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_heads </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> d_v</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">input_Q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">input_K</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">input_V</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        input_Q: [batch_size, len_q, d_model]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        input_K: [batch_size, len_k, d_model]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        input_V: [batch_size, len_v(=len_k), d_model]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        attn_mask: [batch_size, seq_len, seq_len]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 残差</span></span>
<span class="line"><span style="color:#BABED8;">        residual</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> batch_size </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> input_Q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> input_Q</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        Q </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">W_Q</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_Q</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                                   n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        K </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">W_K</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_K</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                                   n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        V </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">W_V</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_V</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                                   n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> attn_mask</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">repeat</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 实例化-&gt;传递参数</span></span>
<span class="line"><span style="color:#BABED8;">        context</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">ScaledDotProductAttention</span><span style="color:#89DDFF;">()(</span><span style="color:#82AAFF;">Q</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> K</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> V</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> attn_mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        context </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> context</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">reshape</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_heads </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> d_v</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 全连接映射成一维矩阵</span></span>
<span class="line"><span style="color:#BABED8;">        output </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">fc</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">context</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 残差</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">LayerNorm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()(</span><span style="color:#82AAFF;">output </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> residual</span><span style="color:#89DDFF;">),</span><span style="color:#BABED8;"> attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><h4 id="tip6-torch-matmul" tabindex="-1">Tip6 <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html" target="_blank" rel="noreferrer">torch.matmul()</a> <a class="header-anchor" href="#tip6-torch-matmul" aria-hidden="true">#</a></h4><p>Matrix product of two tensors 代码示例👇</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">6</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">reshape</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">b </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">6</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">reshape</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">c </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> b</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> b</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> c</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><pre><code>(tensor([[0, 1, 2],
         [3, 4, 5]]),
 tensor([[0, 1],
         [2, 3],
         [4, 5]]),
 tensor([[10, 13],
         [28, 40]]))
</code></pre><h4 id="tip7-masked-fill" tabindex="-1">Tip7 <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" target="_blank" rel="noreferrer">masked_fill()</a> <a class="header-anchor" href="#tip7-masked-fill" aria-hidden="true">#</a></h4><p>Fills elements of self tensor with value where mask is True.<br> The shape of mask must be broadcastable with the shape of the underlying tensor.</p><h4 id="step5-feedforward-layer" tabindex="-1">Step5 FeedForward Layer <a class="header-anchor" href="#step5-feedforward-layer" aria-hidden="true">#</a></h4><p>前馈神经网络<br> 两次线性变换，RELU 作激活层<br> 残差连接（防止原始数据丢失）</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">fc</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Sequential</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_ff</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False),</span></span>
<span class="line"><span style="color:#82AAFF;">            nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ReLU</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">            nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_ff</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">inputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 残差保存原始输入</span></span>
<span class="line"><span style="color:#BABED8;">        residual </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> inputs</span></span>
<span class="line"><span style="color:#BABED8;">        output </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">fc</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">LayerNorm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()(</span><span style="color:#82AAFF;">output </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> residual</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h4 id="step6-encoder" tabindex="-1">Step6 Encoder <a class="header-anchor" href="#step6-encoder" aria-hidden="true">#</a></h4><p>self-attention and feedforward_layer</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">EncoderLayer</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">EncoderLayer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">enc_self_attn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_ffn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_self_attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># K, Q, V, attn_mask</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">enc_self_attn</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_self_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_ffn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">Encoder</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">Encoder Block</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">Encoder</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 词嵌入</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">src_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Embedding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">src_vocab_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 位置编码</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PositionalEncoding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># ? 模块列表，包含多个编码器层</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ModuleList</span><span style="color:#89DDFF;">([</span><span style="color:#82AAFF;">EncoderLayer</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#82AAFF;"> _ </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#82AAFF;"> range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_layers</span><span style="color:#89DDFF;">)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">src_emb</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_emb</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_self_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_self_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[]</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 循环遍历每一个编码器层，将词向量和自注意力掩码传递给每一个层，获取该层的输出及自注意力权重，并存储在列表中</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> layer </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#BABED8;">            enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">layer</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_self_attn_mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">            enc_self_attns</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_self_attn</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attns</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><h4 id="step7-decoder" tabindex="-1">Step7 Decoder <a class="header-anchor" href="#step7-decoder" aria-hidden="true">#</a></h4><p>Masked Multihead attention Multihead attention Feedforward network</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">DecoderLayer</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">DecoderLayer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">dec_self_attn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">dec_enc_attn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_ffn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_self_attn_mask</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_enc_attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dec_self_attn</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_self_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># ?将 dec_outputs 作为生成 Q 的元素，enc_outputs 作为生成 K 和 V 的元素</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dec_enc_attn</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_enc_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_ffn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attn</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">Decoder</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">Decoder</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">tgt_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Embedding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tgt_vocab_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PositionalEncoding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ModuleList</span><span style="color:#89DDFF;">([</span><span style="color:#82AAFF;">DecoderLayer</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#82AAFF;"> _ </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#82AAFF;"> range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_layers</span><span style="color:#89DDFF;">)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_outputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tgt_emb</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_emb</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attn_pad_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attn_subsequence_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_subsequence_mask</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># torch.gt(a, value),将 a 中各个位置上的元素和 value 比较，若大于 value，则该位置取 1，否则取 0</span></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">gt</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_self_attn_pad_mask </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> dec_self_attn_subsequence_mask</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">0</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_enc_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[],</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[]</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> layer </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#BABED8;">            dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attn</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">layer</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">                dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_self_attn_mask</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_enc_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">            dec_self_attns</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_self_attn</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">            dec_enc_attns</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_enc_attn</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><h4 id="tip7-torch-gt" tabindex="-1">Tip7 torch.gt() <a class="header-anchor" href="#tip7-torch-gt" aria-hidden="true">#</a></h4><p>Computes input &gt; element-wise<br> torch.gt(a, value) 的意思是，将 a 中各个位置上的元素和 value 比较， 若大于 value，则该位置取 1，否则取 0</p><h4 id="step8-transformer" tabindex="-1">Step8 Transformer <a class="header-anchor" href="#step8-transformer" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">Transformer</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">Transformer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">encoder</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">Encoder</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">decoder</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">Decoder</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">projection</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tgt_vocab_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_inputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">encoder</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> s </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">decoder</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        dec_logits </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">projection</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#BABED8;">            dec_logits</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_logits</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)),</span></span>
<span class="line"><span style="color:#BABED8;">            enc_self_attns</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#BABED8;">            dec_self_attns</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#BABED8;">            dec_enc_attns</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><h4 id="step9-损失函数-优化器" tabindex="-1">Step9 损失函数，优化器 <a class="header-anchor" href="#step9-损失函数-优化器" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#BABED8;">model </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">Transformer</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># ignore_index=0,不计算pad的损失</span></span>
<span class="line"><span style="color:#BABED8;">criterion </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">CrossEntropyLoss</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">ignore_index</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">optimizer </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">optim</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">SGD</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">parameters</span><span style="color:#89DDFF;">(),</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">lr</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">1e-3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">momentum</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0.99</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h4 id="step11-训练" tabindex="-1">Step11 训练 <a class="header-anchor" href="#step11-训练" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> epoch </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">30</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_outputs </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> loader</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 存储到gpu中</span></span>
<span class="line"><span style="color:#BABED8;">        enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#BABED8;">            enc_inputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#BABED8;">            dec_inputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#BABED8;">            dec_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        loss </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">criterion</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Epoch:</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#F78C6C;">%04d</span><span style="color:#89DDFF;">&quot;</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">%</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">epoch </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">loss =</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#F78C6C;">{</span><span style="color:#C792EA;">:.6f</span><span style="color:#F78C6C;">}</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">loss</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        optimizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zero_grad</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        loss</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">backward</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        optimizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">step</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><hr><p><strong>引用</strong><br><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">Attension is all you need</a><br> 源于<a href="https://www.bilibili.com/video/BV1mk4y1q7eK/?spm_id_from=333.999.0.0&amp;vd_source=744197c073f4828379c29fa20f3ea477" target="_blank" rel="noreferrer">大佬视频讲解</a><br> 本 blog 的<a href="https://github.com/CyanCat22/transfomer_mini_/tree/main/realize" target="_blank" rel="noreferrer">代码链接</a><br><a href="https://github.com/km1994/nlp_paper_study_transformer/tree/main/DL_algorithm/transformer_study/Transformer" target="_blank" rel="noreferrer">杨夕的 Transformer repo</a><br> 分享一个<a href="https://www.zhihu.com/question/556300395" target="_blank" rel="noreferrer">jupyter 转 md</a></p></div></div></main><!--[--><!--]--><footer class="VPDocFooter" data-v-ae7d01fc data-v-2813752b><div class="edit-info" data-v-2813752b><!----><div class="last-updated" data-v-2813752b><p class="VPLastUpdated" data-v-2813752b data-v-355aa5ef>上次更新于: <time datetime="2023-12-12T04:01:00.000Z" data-v-355aa5ef></time></p></div></div><!----></footer><!--[--><!--[--><!--[--><div class="comment" id="giscus-comment" data-pagefind-ignore="all" data-v-a71b6ee6 data-v-8ec3e109><div class="el-affix comment-btn" style="height:;width:;" data-v-8ec3e109><div class="" style=""><!--[--><button ariadisabled="false" type="button" class="el-button el-button--primary is-plain" style="" data-v-8ec3e109><i class="el-icon" style=""><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path fill="currentColor" d="M736 504a56 56 0 1 1 0-112 56 56 0 0 1 0 112m-224 0a56 56 0 1 1 0-112 56 56 0 0 1 0 112m-224 0a56 56 0 1 1 0-112 56 56 0 0 1 0 112M128 128v640h192v160l224-160h352V128z"></path></svg><!--]--></i><span class=""><!--[-->评论<!--]--></span></button><!--]--></div></div><!----></div><!--]--><!--]--><!--]--></div></div></div></div></div><footer class="VPFooter has-sidebar" data-v-fab3d334 data-v-d24360a6><div class="container" data-v-d24360a6><!----><p class="copyright" data-v-d24360a6>不知江月待何人🌱</p></div></footer><!--[--><!--]--></div></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"index.md\":\"4e77a008\",\"src_bert_bert.md\":\"59fd7f77\",\"src_transformer_01_trans_learn_note.md\":\"767e6bd7\",\"about.md\":\"e3505837\",\"src_gat_gat.md\":\"15c780bf\"}")
__VP_SITE_DATA__ = JSON.parse("{\"lang\":\"zh-cmn-Hans\",\"dir\":\"ltr\",\"title\":\"Gua's Blog\",\"description\":\"呱呱的博客, GuaGua blog\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"blog\":{\"pagesData\":[{\"route\":\"/about\",\"meta\":{\"hidden\":true,\"title\":\"关于我:rose:\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"你好！  \\n窝系:star:呱呱:star:  \\n一个快乐阳光滴 girl:stuck_out_tongue_closed_eyes:  \\n我会在这里抒写呱呱的编程之旅，希望能帮助到你~  \\n联系我:\",\"cover\":\"\"}},{\"route\":\"/src/Bert/Bert\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"B\",\"7\":\"e\",\"8\":\"r\",\"9\":\"t\",\"10\":\"_\",\"11\":\"l\",\"12\":\"e\",\"13\":\"a\",\"14\":\"r\",\"15\":\"n\",\"16\":\"_\",\"17\":\"n\",\"18\":\"o\",\"19\":\"t\",\"20\":\"e\",\"21\":\"1\",\"22\":\" \",\"23\":\"t\",\"24\":\"a\",\"25\":\"g\",\"26\":\"s\",\"27\":\":\",\"28\":\"[\",\"29\":\"B\",\"30\":\"e\",\"31\":\"r\",\"32\":\"t\",\"33\":\",\",\"34\":\" \",\"35\":\"n\",\"36\":\"l\",\"37\":\"p\",\"38\":\"]\",\"title\":\"Bert学习笔记🌟\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"全称：Bidirectinal Encoder Representation from Transformer  \\n是 Google 以无监督的方式利用大量无标注的文本练成的语言模型\\nGoogle 在\",\"cover\":\"\"}},{\"route\":\"/src/GAT/GAT\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"G\",\"7\":\"A\",\"8\":\"T\",\"9\":\" \",\"10\":\"学\",\"11\":\"习\",\"12\":\"笔\",\"13\":\"记\",\"14\":\" \",\"15\":\"t\",\"16\":\"a\",\"17\":\"g\",\"18\":\"s\",\"19\":\":\",\"20\":\"[\",\"21\":\"G\",\"22\":\"A\",\"23\":\"T\",\"24\":\",\",\"25\":\" \",\"26\":\"n\",\"27\":\"l\",\"28\":\"p\",\"29\":\"]\",\"title\":\"GAT图神经网络学习笔记\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"_关于图、GAT 的学习记录_\\n_为何叫 GAT 捏，因为 GAN 一般指的是 Generative Adversal Nets_\\n 数据结构-图\\n图的三个特征\\n- `node` 节点，每个顶点有着自\",\"cover\":\"https://pic1.zhimg.com/80/v2-ec415ca61d7eef27296aff1994e91db8_1440w.webp\"}},{\"route\":\"/src/Transformer/01/trans_learn_note\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"T\",\"7\":\"r\",\"8\":\"a\",\"9\":\"n\",\"10\":\"s\",\"11\":\"f\",\"12\":\"o\",\"13\":\"m\",\"14\":\"e\",\"15\":\"r\",\"16\":\"_\",\"17\":\"n\",\"18\":\"o\",\"19\":\"t\",\"20\":\"e\",\"21\":\" \",\"22\":\"t\",\"23\":\"a\",\"24\":\"g\",\"25\":\"s\",\"26\":\":\",\"27\":\"[\",\"28\":\"T\",\"29\":\"r\",\"30\":\"a\",\"31\":\"n\",\"32\":\"s\",\"33\":\"f\",\"34\":\"o\",\"35\":\"r\",\"36\":\"m\",\"37\":\"e\",\"38\":\"r\",\"39\":\",\",\"40\":\"n\",\"41\":\"l\",\"42\":\"p\",\"43\":\"]\",\"title\":\"🌟Transformer学习，实现🌟\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"本文是初学 transformer 的笔记记录、代码实现  \\n还有 pytorch 库中一些函数的用法 tips :\\ntransformer 框架图:point_down:\\n 代码实现\\n 库&参数\\n`\",\"cover\":\"\"}}],\"author\":\"GuaGua\",\"friend\":[{\"nickname\":\"GuaGua's Blog\",\"des\":\"呼噜噜～(￣▽￣～)~\",\"avatar\":\"/avatar.jpg\",\"url\":\"/about\"}],\"comment\":{\"repo\":\"CyanCat22/CyanCat22.github.io\",\"repoId\":\"R_kgDOJONNeg\",\"category\":\"Ideas\",\"categoryId\":\"DIC_kwDOJONNes4CVJhx\",\"mapping\":\"title\",\"loading\":\"lazy\",\"inputPosition\":\"top\"},\"search\":\"pagefind\"},\"sidebar\":[{\"text\":\"\",\"items\":[]}],\"lastUpdatedText\":\"上次更新于\",\"footer\":{\"copyright\":\"不知江月待何人🌱\"},\"logo\":\"https://avatars.githubusercontent.com/u/122679149?v=4\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/CyanCat22\"}]},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}")</script>
    
  </body>
</html>