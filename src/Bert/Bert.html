<!DOCTYPE html>
<html lang="zh-cmn-Hans" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Bert 学习笔记 🌟 | Gua's Blog</title>
    <meta name="description" content="呱呱的博客, GuaGua blog">
    <link rel="preload stylesheet" href="/assets/style.aec4d03c.css" as="style">
    <script type="module" src="/assets/app.e0ff3824.js"></script>
    <link rel="modulepreload" href="/assets/chunks/framework.11961b2a.js">
    <link rel="modulepreload" href="/assets/chunks/theme.0b6af2c9.js">
    <link rel="modulepreload" href="/assets/src_Bert_Bert.md.59fd7f77.lean.js">
    
    <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body><!--v-if--><!--teleport anchor-->
    <div id="app"><div class="Layout" data-v-a71b6ee6 data-v-fab3d334><!--[--><!----><!--[--><div class="theme-blog-popover" style="display:none;" data-pagefind-ignore="all" data-v-f1f6e4fb><div class="header" data-v-f1f6e4fb><div class="title-wrapper" data-v-f1f6e4fb><i class="el-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M288 128h608L736 384l160 256H288v320h-96V64h96z"></path></svg><!--]--></i><span class="title" data-v-f1f6e4fb></span></div><i class="el-icon close-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M512 64a448 448 0 1 1 0 896 448 448 0 0 1 0-896m0 393.664L407.936 353.6a38.4 38.4 0 1 0-54.336 54.336L457.664 512 353.6 616.064a38.4 38.4 0 1 0 54.336 54.336L512 566.336 616.064 670.4a38.4 38.4 0 1 0 54.336-54.336L566.336 512 670.4 407.936a38.4 38.4 0 1 0-54.336-54.336z"></path></svg><!--]--></i></div><!----><div class="footer content" data-v-f1f6e4fb><!--[--><!--]--></div></div><div class="theme-blog-popover-close" style="display:none;" data-v-f1f6e4fb><i class="el-icon" style="font-size:20px;" data-v-f1f6e4fb><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-f1f6e4fb><path fill="currentColor" d="M288 128h608L736 384l160 256H288v320h-96V64h96z"></path></svg><!--]--></i></div><!--]--><!--]--><!--[--><span tabindex="-1" data-v-151f2593></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-151f2593> Skip to content </a><!--]--><!----><header class="VPNav" data-v-fab3d334 data-v-0fa0e57d><div class="VPNavBar has-sidebar" data-v-0fa0e57d data-v-be450ad9><div class="container" data-v-be450ad9><div class="title" data-v-be450ad9><div class="VPNavBarTitle has-sidebar" data-v-be450ad9 data-v-6d2fb2d9><a class="title" href="/" data-v-6d2fb2d9><!--[--><!--]--><!--[--><img class="VPImage logo" src="https://avatars.githubusercontent.com/u/122679149?v=4" alt data-v-6db2186b><!--]--><!--[-->Gua&#39;s Blog<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-be450ad9><div class="curtain" data-v-be450ad9></div><div class="content-body" data-v-be450ad9><!--[--><!--[--><!--[--><div class="blog-search" data-pagefind-ignore="all" data-v-a71b6ee6 style="--4c3f391a:1;" data-v-3ff534af><div class="nav-search-btn-wait" data-v-3ff534af><svg width="14" height="14" viewBox="0 0 20 20" data-v-3ff534af><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round" data-v-3ff534af></path></svg><span class="search-tip" data-v-3ff534af>搜索</span><span class="metaKey" data-v-3ff534af> K </span></div><!--teleport start--><!--teleport end--></div><!--]--><!--]--><!--]--><!----><!----><!----><div class="VPNavBarAppearance appearance" data-v-be450ad9 data-v-da3f667a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-da3f667a data-v-0d529b6d data-v-f3c41672><span class="check" data-v-f3c41672><span class="icon" data-v-f3c41672><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0d529b6d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0d529b6d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-be450ad9 data-v-2ab2a029 data-v-f6988cfb><!--[--><a class="VPSocialLink" href="https://github.com/CyanCat22" target="_blank" rel="noopener" data-v-f6988cfb data-v-e57698f6><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-be450ad9 data-v-66bb1f24 data-v-96001b6b><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-96001b6b><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-96001b6b><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-96001b6b><div class="VPMenu" data-v-96001b6b data-v-e7ea1737><!----><!--[--><!--[--><!----><div class="group" data-v-66bb1f24><div class="item appearance" data-v-66bb1f24><p class="label" data-v-66bb1f24>Appearance</p><div class="appearance-action" data-v-66bb1f24><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-66bb1f24 data-v-0d529b6d data-v-f3c41672><span class="check" data-v-f3c41672><span class="icon" data-v-f3c41672><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0d529b6d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0d529b6d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-66bb1f24><div class="item social-links" data-v-66bb1f24><div class="VPSocialLinks social-links-list" data-v-66bb1f24 data-v-f6988cfb><!--[--><a class="VPSocialLink" href="https://github.com/CyanCat22" target="_blank" rel="noopener" data-v-f6988cfb data-v-e57698f6><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-be450ad9 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav" data-v-fab3d334 data-v-2817d72e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2817d72e><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-2817d72e><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-2817d72e>Menu</span></button><a class="top-link" href="#" data-v-2817d72e>Return to top</a></div><aside class="VPSidebar" data-v-fab3d334 data-v-c79ccefa><div class="curtain" data-v-c79ccefa></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-c79ccefa><span class="visually-hidden" id="sidebar-aria-label" data-v-c79ccefa> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-c79ccefa><div class="VPSidebarItem level-0" data-v-c79ccefa data-v-983f6b21><!----><!----></div></div><!--]--><!--[--><!--[--><div class="sidebar" data-pagefind-ignore="all" data-v-a71b6ee6 data-v-e42837f7><div class="card recommend" data-pagefind-ignore="all" data-v-e42837f7 data-v-2844b86b><div class="card-header" data-v-2844b86b><span class="title" data-v-2844b86b>🔍 相关文章</span><!----></div><div class="empty-text" data-v-2844b86b>暂无推荐文章</div></div></div><!--]--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-pagefind-body data-v-fab3d334 data-v-fe5dac9b><div class="VPDoc has-sidebar has-aside" data-v-fe5dac9b data-v-ae7d01fc><div class="container" data-v-ae7d01fc><div class="aside" data-v-ae7d01fc><div class="aside-curtain" data-v-ae7d01fc></div><div class="aside-container" data-v-ae7d01fc><div class="aside-content" data-v-ae7d01fc><div class="VPDocAside" data-v-ae7d01fc data-v-cdc66372><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-cdc66372 data-v-6106c300><div class="content" data-v-6106c300><div class="outline-marker" data-v-6106c300></div><div class="outline-title" data-v-6106c300>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-6106c300><span class="visually-hidden" id="doc-outline-aria-label" data-v-6106c300> Table of Contents for current page </span><ul class="root" data-v-6106c300 data-v-1188541a><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-cdc66372></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-ae7d01fc><div class="content-container" data-v-ae7d01fc><!--[--><!--[--><!--[--><!----><!----><!--]--><!--]--><!--]--><main class="main" data-v-ae7d01fc><div style="position:relative;" class="vp-doc _src_Bert_Bert" data-v-ae7d01fc><div><h1 id="bert-学习笔记-🌟" tabindex="-1">Bert 学习笔记 🌟 <a class="header-anchor" href="#bert-学习笔记-🌟" aria-hidden="true">#</a></h1><p>全称：<strong>Bidirectinal Encoder Representation from Transformer</strong><br> 是 Google 以无监督的方式利用大量<strong>无标注</strong>的文本练成的<strong>语言模型</strong></p><p>Google 在训练 Bert 时让他同时进行两个预训练任务：</p><ol><li>完形填空 Masked Language Model</li><li>判断第 2 个句子在原始文本中与第 1 个是否相接 Next Sentence Prediction</li></ol><p><img src="/assets/bert_.e4867915.jpg" alt=""></p><h2 id="语言模型-lm-的好处" tabindex="-1">语言模型 LM 的好处 <a class="header-anchor" href="#语言模型-lm-的好处" aria-hidden="true">#</a></h2><ul><li>无监督数据无限大，不用找人来标注数据</li><li>能顾学习语法结构、解读语义和指代消解（Coreference Resolution），能更有效的训练下游任务并提高其表现</li><li>减少处理不同 NLP 任务所需 architecture engineering 成本（人力、时间、计算资源）</li></ul><h2 id="迁移学习" tabindex="-1">迁移学习 <a class="header-anchor" href="#迁移学习" aria-hidden="true">#</a></h2><ul><li>先以 LM Pretraining 的方式预先训练出一个对自然语言有一定「理解」的通用模型</li><li>再将该模型拿來做特征提取或是 fine tune 下游的（监督式）任务</li></ul><h2 id="bert-预训练任务一-masked-language-model" tabindex="-1">Bert 预训练任务一：Masked Language Model <a class="header-anchor" href="#bert-预训练任务一-masked-language-model" aria-hidden="true">#</a></h2><p>先用 transfomers 中的 bert 预训练模型感受一下哈~</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#BABED8;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> BertTokenizer</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#BABED8;"> IPython</span><span style="color:#89DDFF;">.</span><span style="color:#BABED8;">display </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> clear_output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">使用中文 BERT-BASE 预训练模型 Use Bert!</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">PRETRAINED_MODEL_NAME </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">bert-base-chinese</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 取得此预训练模型所使用的 tokenizer</span></span>
<span class="line"><span style="color:#BABED8;">tokenizer </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> BertTokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PRETRAINED_MODEL_NAME</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#82AAFF;">clear_output</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">vocab </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">vocab</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">字典大小</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> len</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">vocab</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#BABED8;">text </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">[CLS] 等到潮水 [MASK] 了，就知道谁没穿裤子。</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#BABED8;">tokens </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tokenize</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">text</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">ids </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">convert_tokens_to_ids</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">text</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ids</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;[MASK]&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">载入已经训练好的 masked 语言模型并对有 [MASK] 的句子做预测</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#BABED8;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> BertForMaskedLM</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 除了 tokens 以外我們还需要辨別句子的 segment ids</span></span>
<span class="line"><span style="color:#BABED8;">tokens_tensor </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tensor</span><span style="color:#89DDFF;">([</span><span style="color:#82AAFF;">ids</span><span style="color:#89DDFF;">])</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># (1, seq_len)</span></span>
<span class="line"><span style="color:#BABED8;">segments_tensors </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros_like</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens_tensor</span><span style="color:#89DDFF;">)</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># (1, seq_len)</span></span>
<span class="line"><span style="color:#BABED8;">maskedLM_model </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> BertForMaskedLM</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PRETRAINED_MODEL_NAME</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">clear_output</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 使用 masked LM 估计 [MASK] 位置所代表的实际 token</span></span>
<span class="line"><span style="color:#BABED8;">maskedLM_model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">eval</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">with</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">no_grad</span><span style="color:#89DDFF;">():</span></span>
<span class="line"><span style="color:#BABED8;">    outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">maskedLM_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens_tensor</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> segments_tensors</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    predictions </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> outputs</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># (1, seq_len, num_hidden_units)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">del</span><span style="color:#BABED8;"> maskedLM_model</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 将 [MASK] 位置的分布取 top k 最有可能的 tokens 出來</span></span>
<span class="line"><span style="color:#BABED8;">masked_index </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">5</span></span>
<span class="line"><span style="color:#BABED8;">k </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">3</span></span>
<span class="line"><span style="color:#BABED8;">probs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> indices </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">topk</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">softmax</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">predictions</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> masked_index</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> k</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">predicted_tokens </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">convert_ids_to_tokens</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">indices</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tolist</span><span style="color:#89DDFF;">())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 我们取 top 1 当做预测值</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">输入 tokens ：</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokens</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">-</span><span style="color:#89DDFF;">&quot;</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">50</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> i</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;">t</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> p</span><span style="color:#89DDFF;">)</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">enumerate</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">zip</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">predicted_tokens</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> probs</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    tokens</span><span style="color:#89DDFF;">[</span><span style="color:#BABED8;">masked_index</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> t</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Top </span><span style="color:#F78C6C;">{}</span><span style="color:#C3E88D;"> (</span><span style="color:#F78C6C;">{</span><span style="color:#C792EA;">:2</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">%)：</span><span style="color:#F78C6C;">{}</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        i</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#FFCB6B;">int</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">p</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">item</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> tokens</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">]),</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">輸入 tokens ： [&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;[MASK]&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">--------------------------------------------------</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Top 1 (82%)：[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;來&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Top 2 (11%)：[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;濕&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Top 3 ( 2%)：[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;過&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="tokens" tabindex="-1">Tokens <a class="header-anchor" href="#tokens" aria-hidden="true">#</a></h3><p><code>[CLS]</code> 输入序列的 repr., 一般放在句子开头<br><code>[SEP]</code> 分隔符 两个句子的交界，在两个句子之间或是句子末尾<br><code>[UNK]</code> 未出现的词汇 (unknown)<br><code>[PAD]</code> zero_padding 将长度不一样的序列补齐方便做 batch 运算<br><code>[MASK]</code> 未知屏蔽，仅在预训练阶段会用到</p><h3 id="实现细节" tabindex="-1">实现细节 <a class="header-anchor" href="#实现细节" aria-hidden="true">#</a></h3><p>随机遮盖或替换一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后做 Loss 的时候也只计算被遮盖部分的 Loss</p><ol><li><p>随机把一句话中 15% 的 token（字或词）替换成以下内容：<br> a. 这些 token 有 80% 的几率被替换成 [MASK]，例如 my dog is hairy→my dog is [MASK]<br> b. 有 10% 的几率被替换成任意一个其它的 token，例如 my dog is hairy→my dog is apple<br> c. 有 10% 的几率原封不动，例如 my dog is hairy→my dog is hairy</p></li><li><p>之后让模型预测和还原被遮盖掉或替换掉的部分，计算损失的时候，只计算在第 1 步里被随机遮盖或替换的部分，其余部分不做损失，其余部分无论输出什么东西，都无所谓</p></li></ol><p>这样做的好处是，BERT 并不知道 [MASK] 替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。 这样可以强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行 &quot;纠错&quot;。比如在上面的例子中，模型在编码 apple 时，根据上下文 my dog is，应该把 apple 编码成 hairy 的语义而不是 apple 的语义</p><h2 id="bert-预训练任务二-next-sentence-prediction" tabindex="-1">Bert 预训练任务二：Next Sentence Prediction <a class="header-anchor" href="#bert-预训练任务二-next-sentence-prediction" aria-hidden="true">#</a></h2><blockquote><p>LML(完型填空)能让 Bert 更好的 model 每个词汇在不同语境下的 repr.，而 NSP 任务则能帮助 Bert model 两个句子之间的关系，这在问答系统 QA、自然语言推论 NLI 任务有很大帮助</p></blockquote><p><img src="/assets/NSP.41e9038c.png" alt=""></p><ul><li>Token Embedding 就是正常的词向量，即 nn.Embedding()</li><li>Segment Embedding 作用是用 embedding 的信息让模型分开上下句，给上句的 token 全 0，下句的 token 全 1，让模型得以判断上下句的起止位置<br> [CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]<br> 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1</li><li>Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的</li></ul><p>BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加</p><h2 id="elmo-bert-gpt" tabindex="-1">ELMo, Bert, Gpt <a class="header-anchor" href="#elmo-bert-gpt" aria-hidden="true">#</a></h2><p><img src="/assets/Difference.8276710b.png" alt=""></p><ul><li><p><a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noreferrer">ELMo</a>(Embedding from Languang model) 利用独立训练的<strong>双向两层 LSTM</strong> 做语言模型并将中间得到的隐状态向量串接当作每个词汇的 contextual word repr.</p></li><li><p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noreferrer">GPT</a>(Generative Pre-Training) 则是使用 <strong>Transformer 的 Decoder</strong> 来训练一个中规中矩，从左到右的单向语言模型，使用的数据、参数量庞大</p></li><li><p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noreferrer">BERT</a> 跟它们的差异在于利用<strong>MLM（克漏字）<strong>的概念及</strong>Transformer Encoder</strong>的架构，摆脱以往语言模型只能从单个方向（由左到右或由右到左）估计下个词汇的出现几率，训练出一个双向的语言代表模型。 这使得 BERT 输出的每个 token 的 repr. 都同时蕴含了前后文信息，实现了真正的双向 representation</p></li></ul><hr><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-hidden="true">#</a></h2><p><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" target="_blank" rel="noreferrer">leemeng Bert 巨人之力</a><br><a href="https://wmathor.com/index.php/archives/1456/" target="_blank" rel="noreferrer">Bert note</a><br><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html" target="_blank" rel="noreferrer">GPT 生成金庸小说</a><br><a href="https://github.com/leemengtw/deep-learning-resources" target="_blank" rel="noreferrer">深度学习资源 repo </a><br><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html" target="_blank" rel="noreferrer">李宏毅 ML 课程</a><br><a href="https://github.com/km1994/nlp_paper_study_transformer/tree/main/DL_algorithm/transformer_study/Transformer" target="_blank" rel="noreferrer">杨夕大佬的 repo</a></p></div></div></main><!--[--><!--]--><footer class="VPDocFooter" data-v-ae7d01fc data-v-2813752b><div class="edit-info" data-v-2813752b><!----><div class="last-updated" data-v-2813752b><p class="VPLastUpdated" data-v-2813752b data-v-355aa5ef>上次更新于: <time datetime="2023-12-12T04:01:00.000Z" data-v-355aa5ef></time></p></div></div><!----></footer><!--[--><!--[--><!--[--><div class="comment" id="giscus-comment" data-pagefind-ignore="all" data-v-a71b6ee6 data-v-8ec3e109><div class="el-affix comment-btn" style="height:;width:;" data-v-8ec3e109><div class="" style=""><!--[--><button ariadisabled="false" type="button" class="el-button el-button--primary is-plain" style="" data-v-8ec3e109><i class="el-icon" style=""><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path fill="currentColor" d="M736 504a56 56 0 1 1 0-112 56 56 0 0 1 0 112m-224 0a56 56 0 1 1 0-112 56 56 0 0 1 0 112m-224 0a56 56 0 1 1 0-112 56 56 0 0 1 0 112M128 128v640h192v160l224-160h352V128z"></path></svg><!--]--></i><span class=""><!--[-->评论<!--]--></span></button><!--]--></div></div><!----></div><!--]--><!--]--><!--]--></div></div></div></div></div><footer class="VPFooter has-sidebar" data-v-fab3d334 data-v-d24360a6><div class="container" data-v-d24360a6><!----><p class="copyright" data-v-d24360a6>不知江月待何人🌱</p></div></footer><!--[--><!--]--></div></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"index.md\":\"4e77a008\",\"src_bert_bert.md\":\"59fd7f77\",\"src_transformer_01_trans_learn_note.md\":\"767e6bd7\",\"about.md\":\"e3505837\",\"src_gat_gat.md\":\"15c780bf\"}")
__VP_SITE_DATA__ = JSON.parse("{\"lang\":\"zh-cmn-Hans\",\"dir\":\"ltr\",\"title\":\"Gua's Blog\",\"description\":\"呱呱的博客, GuaGua blog\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"blog\":{\"pagesData\":[{\"route\":\"/about\",\"meta\":{\"hidden\":true,\"title\":\"关于我:rose:\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"你好！  \\n窝系:star:呱呱:star:  \\n一个快乐阳光滴 girl:stuck_out_tongue_closed_eyes:  \\n我会在这里抒写呱呱的编程之旅，希望能帮助到你~  \\n联系我:\",\"cover\":\"\"}},{\"route\":\"/src/Bert/Bert\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"B\",\"7\":\"e\",\"8\":\"r\",\"9\":\"t\",\"10\":\"_\",\"11\":\"l\",\"12\":\"e\",\"13\":\"a\",\"14\":\"r\",\"15\":\"n\",\"16\":\"_\",\"17\":\"n\",\"18\":\"o\",\"19\":\"t\",\"20\":\"e\",\"21\":\"1\",\"22\":\" \",\"23\":\"t\",\"24\":\"a\",\"25\":\"g\",\"26\":\"s\",\"27\":\":\",\"28\":\"[\",\"29\":\"B\",\"30\":\"e\",\"31\":\"r\",\"32\":\"t\",\"33\":\",\",\"34\":\" \",\"35\":\"n\",\"36\":\"l\",\"37\":\"p\",\"38\":\"]\",\"title\":\"Bert学习笔记🌟\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"全称：Bidirectinal Encoder Representation from Transformer  \\n是 Google 以无监督的方式利用大量无标注的文本练成的语言模型\\nGoogle 在\",\"cover\":\"\"}},{\"route\":\"/src/GAT/GAT\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"G\",\"7\":\"A\",\"8\":\"T\",\"9\":\" \",\"10\":\"学\",\"11\":\"习\",\"12\":\"笔\",\"13\":\"记\",\"14\":\" \",\"15\":\"t\",\"16\":\"a\",\"17\":\"g\",\"18\":\"s\",\"19\":\":\",\"20\":\"[\",\"21\":\"G\",\"22\":\"A\",\"23\":\"T\",\"24\":\",\",\"25\":\" \",\"26\":\"n\",\"27\":\"l\",\"28\":\"p\",\"29\":\"]\",\"title\":\"GAT图神经网络学习笔记\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"_关于图、GAT 的学习记录_\\n_为何叫 GAT 捏，因为 GAN 一般指的是 Generative Adversal Nets_\\n 数据结构-图\\n图的三个特征\\n- `node` 节点，每个顶点有着自\",\"cover\":\"https://pic1.zhimg.com/80/v2-ec415ca61d7eef27296aff1994e91db8_1440w.webp\"}},{\"route\":\"/src/Transformer/01/trans_learn_note\",\"meta\":{\"0\":\"t\",\"1\":\"i\",\"2\":\"t\",\"3\":\"l\",\"4\":\"e\",\"5\":\":\",\"6\":\"T\",\"7\":\"r\",\"8\":\"a\",\"9\":\"n\",\"10\":\"s\",\"11\":\"f\",\"12\":\"o\",\"13\":\"m\",\"14\":\"e\",\"15\":\"r\",\"16\":\"_\",\"17\":\"n\",\"18\":\"o\",\"19\":\"t\",\"20\":\"e\",\"21\":\" \",\"22\":\"t\",\"23\":\"a\",\"24\":\"g\",\"25\":\"s\",\"26\":\":\",\"27\":\"[\",\"28\":\"T\",\"29\":\"r\",\"30\":\"a\",\"31\":\"n\",\"32\":\"s\",\"33\":\"f\",\"34\":\"o\",\"35\":\"r\",\"36\":\"m\",\"37\":\"e\",\"38\":\"r\",\"39\":\",\",\"40\":\"n\",\"41\":\"l\",\"42\":\"p\",\"43\":\"]\",\"title\":\"🌟Transformer学习，实现🌟\",\"date\":\"2023-12-12 12:01:00\",\"tag\":[],\"description\":\"本文是初学 transformer 的笔记记录、代码实现  \\n还有 pytorch 库中一些函数的用法 tips :\\ntransformer 框架图:point_down:\\n 代码实现\\n 库&参数\\n`\",\"cover\":\"\"}}],\"author\":\"GuaGua\",\"friend\":[{\"nickname\":\"GuaGua's Blog\",\"des\":\"呼噜噜～(￣▽￣～)~\",\"avatar\":\"/avatar.jpg\",\"url\":\"/about\"}],\"comment\":{\"repo\":\"CyanCat22/CyanCat22.github.io\",\"repoId\":\"R_kgDOJONNeg\",\"category\":\"Ideas\",\"categoryId\":\"DIC_kwDOJONNes4CVJhx\",\"mapping\":\"title\",\"loading\":\"lazy\",\"inputPosition\":\"top\"},\"search\":\"pagefind\"},\"sidebar\":[{\"text\":\"\",\"items\":[]}],\"lastUpdatedText\":\"上次更新于\",\"footer\":{\"copyright\":\"不知江月待何人🌱\"},\"logo\":\"https://avatars.githubusercontent.com/u/122679149?v=4\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/CyanCat22\"}]},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}")</script>
    
  </body>
</html>