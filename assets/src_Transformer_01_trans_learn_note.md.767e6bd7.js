import{_ as s,o as n,c as a,Q as l}from"./chunks/framework.11961b2a.js";const p="/assets/figure1.97fb2b71.png",o="/assets/position2.1b12b726.png",e="/assets/position.c45c9c7f.png",t="/assets/padding_mask.05b4f1ae.png",r="/assets/attention.d0d7183a.png",c="/assets/self_attention.f815085a.png",_=JSON.parse('{"title":"🌟Transformer 学习，实现 🌟","description":"","frontmatter":{"0":"t","1":"i","2":"t","3":"l","4":"e","5":":","6":"T","7":"r","8":"a","9":"n","10":"s","11":"f","12":"o","13":"m","14":"e","15":"r","16":"_","17":"n","18":"o","19":"t","20":"e","21":" ","22":"t","23":"a","24":"g","25":"s","26":":","27":"[","28":"T","29":"r","30":"a","31":"n","32":"s","33":"f","34":"o","35":"r","36":"m","37":"e","38":"r","39":",","40":"n","41":"l","42":"p","43":"]"},"headers":[{"level":3,"title":"代码实现","slug":"代码实现","link":"#代码实现","children":[{"level":4,"title":"库&参数","slug":"库-参数","link":"#库-参数","children":[]},{"level":4,"title":"Step1 Positional Encoding","slug":"step1-positional-encoding","link":"#step1-positional-encoding","children":[]},{"level":4,"title":"Tip1 torch.unsqueeze()","slug":"tip1-torch-unsqueeze","link":"#tip1-torch-unsqueeze","children":[]},{"level":4,"title":"Tip2 torch.transpose(dim0, dim1)","slug":"tip2-torch-transpose-dim0-dim1","link":"#tip2-torch-transpose-dim0-dim1","children":[]},{"level":4,"title":"Tip3 register_buffer","slug":"tip3-register-buffer","link":"#tip3-register-buffer","children":[]},{"level":4,"title":"Step2 Pad_Mask and Subsequence Mask","slug":"step2-pad-mask-and-subsequence-mask","link":"#step2-pad-mask-and-subsequence-mask","children":[]},{"level":4,"title":"Tip4 torch.eq()","slug":"tip4-torch-eq","link":"#tip4-torch-eq","children":[]},{"level":4,"title":"Tip5 np.triu()","slug":"tip5-np-triu","link":"#tip5-np-triu","children":[]},{"level":4,"title":"Step3 ScaledDotProductAttention","slug":"step3-scaleddotproductattention","link":"#step3-scaleddotproductattention","children":[]},{"level":4,"title":"Step4 MultiHeadAttention","slug":"step4-multiheadattention","link":"#step4-multiheadattention","children":[]},{"level":4,"title":"Tip6 torch.matmul()","slug":"tip6-torch-matmul","link":"#tip6-torch-matmul","children":[]},{"level":4,"title":"Tip7 masked_fill()","slug":"tip7-masked-fill","link":"#tip7-masked-fill","children":[]},{"level":4,"title":"Step5 FeedForward Layer","slug":"step5-feedforward-layer","link":"#step5-feedforward-layer","children":[]},{"level":4,"title":"Step6 Encoder","slug":"step6-encoder","link":"#step6-encoder","children":[]},{"level":4,"title":"Step7 Decoder","slug":"step7-decoder","link":"#step7-decoder","children":[]},{"level":4,"title":"Tip7 torch.gt()","slug":"tip7-torch-gt","link":"#tip7-torch-gt","children":[]},{"level":4,"title":"Step8 Transformer","slug":"step8-transformer","link":"#step8-transformer","children":[]},{"level":4,"title":"Step9 损失函数，优化器","slug":"step9-损失函数-优化器","link":"#step9-损失函数-优化器","children":[]},{"level":4,"title":"Step11 训练","slug":"step11-训练","link":"#step11-训练","children":[]}]}],"relativePath":"src/Transformer/01/trans_learn_note.md","lastUpdated":1702353660000}'),F={name:"src/Transformer/01/trans_learn_note.md"},y=l('<h1 id="🌟transformer-学习-实现-🌟" tabindex="-1">🌟Transformer 学习，实现 🌟 <a class="header-anchor" href="#🌟transformer-学习-实现-🌟" aria-hidden="true">#</a></h1><p>本文是初学 transformer 的笔记记录、代码实现<br> 还有 pytorch 库中一些函数的用法 tips :&gt;</p><p><strong>transformer 框架图👇</strong><img src="'+p+`" alt=""></p><h3 id="代码实现" tabindex="-1">代码实现 <a class="header-anchor" href="#代码实现" aria-hidden="true">#</a></h3><h4 id="库-参数" tabindex="-1">库&amp;参数 <a class="header-anchor" href="#库-参数" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">utils</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">data</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#BABED8;"> Data</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#BABED8;"> nn</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> math</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> numpy </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#BABED8;"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 参数</span></span>
<span class="line"><span style="color:#BABED8;">d_model </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">512</span></span>
<span class="line"><span style="color:#BABED8;">d_ff </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">2048</span></span>
<span class="line"><span style="color:#BABED8;">d_k </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> d_v </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">64</span></span>
<span class="line"><span style="color:#BABED8;">n_layers </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">6</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># number of encoder and decoder layers</span></span>
<span class="line"><span style="color:#BABED8;">n_heads </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">8</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><h4 id="step1-positional-encoding" tabindex="-1">Step1 Positional Encoding <a class="header-anchor" href="#step1-positional-encoding" aria-hidden="true">#</a></h4><p>由于 Transformer 模型没有循环神经网络的迭代操作，所以我们需要提供每个字的<strong>位置信息</strong>给 Transformer，这样它才能识别出语言中的顺序关系<br> 在 transformer 模型里不训练，在 Bert 模型里会进行训练💥 <a href="https://wmathor.com/index.php/archives/1453/" target="_blank" rel="noreferrer">Positional Encoding 文章理解</a></p><ul><li>🔥编码唯一</li><li>🔥值有界</li><li>🔥不同长度的句子之间，任何两个字之间的差值应该一致<br><img src="`+o+'" alt=""><br><img src="'+e+`" alt=""></li></ul><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">PositionalEncoding</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">位置编码</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">d_module</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dropout</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0.1</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">max_len</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">5000</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PositionalEncoding</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">dropout</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Dropout</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">p</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">dropout</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        pe </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">max_len</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_module</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># unsqueeze =&gt; 在指定位置插入一个新的维度</span></span>
<span class="line"><span style="color:#BABED8;">        position </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> max_len</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">dtype</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">float32</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># (seq_len, batch_size, d_model)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 比例因子</span></span>
<span class="line"><span style="color:#BABED8;">        div_term </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">exp</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_module</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">float</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">(-</span><span style="color:#82AAFF;">math</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">log</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10000.0</span><span style="color:#89DDFF;">)</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> d_module</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 0::2 切片表达式，索引从零开始，步长为2（隔一个索引取一个）</span></span>
<span class="line"><span style="color:#BABED8;">        pe</span><span style="color:#89DDFF;">[:,</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">::</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">position </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> div_term</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        pe</span><span style="color:#89DDFF;">[:,</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">::</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cos</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">position </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> div_term</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        pe </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> pe</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 将位置编码矩阵注册为一个缓冲区, 避免在每次前向传播时重新计算位置编码</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">register_buffer</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">pe</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> pe</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        x </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pe</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">:</span><span style="color:#F07178;"> x</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">:]</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dropout</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><h4 id="tip1-torch-unsqueeze" tabindex="-1">Tip1 <a href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html" target="_blank" rel="noreferrer">torch.unsqueeze()</a> <a class="header-anchor" href="#tip1-torch-unsqueeze" aria-hidden="true">#</a></h4><p>=&gt; 在指定位置插入一个维度 扩张维度<br><strong>torch.squeeze()</strong> =&gt; 删除张量中大小为 1 的维度 代码示例👇</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a1 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a2 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a3 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">squeeze</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;向量a4 : </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">, size(a) = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h4 id="tip2-torch-transpose-dim0-dim1" tabindex="-1">Tip2 <a href="https://pytorch.org/docs/stable/generated/torch.transpose.html" target="_blank" rel="noreferrer">torch.transpose(dim0, dim1)</a> <a class="header-anchor" href="#tip2-torch-transpose-dim0-dim1" aria-hidden="true">#</a></h4><p>=&gt; 交换两个指定的维度</p><h4 id="tip3-register-buffer" tabindex="-1">Tip3 register_buffer <a class="header-anchor" href="#tip3-register-buffer" aria-hidden="true">#</a></h4><h4 id="step2-pad-mask-and-subsequence-mask" tabindex="-1">Step2 Pad_Mask and Subsequence Mask <a class="header-anchor" href="#step2-pad-mask-and-subsequence-mask" aria-hidden="true">#</a></h4><p>按照 mini-batch 中最大的句长对剩余的句子进行补齐，一般用 0 进行填充(padding)<br> mask 操作，让无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置</p><p><img src="`+t+`" alt=""></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">seq_q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">seq_k</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> len_q </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> seq_q</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">seq_q</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">())</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">seq_k</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#BABED8;">    batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> len_k </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> seq_k</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># data.eq(0) 是比较操作，找出序列中所有等于零的元素,返回一个True（即填充（PAD）token），False 表示其他非填充元素</span></span>
<span class="line"><span style="color:#BABED8;">    pad_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> seq_k</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">data</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">eq</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># 根据指定的形状参数沿着指定的维度扩展输入张量</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># print(pad_attn_mask.expand(batch_size, len_q, len_k))</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> pad_attn_mask</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">expand</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> len_q</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> len_k</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h4 id="tip4-torch-eq" tabindex="-1">Tip4 <a href="https://pytorch.org/docs/stable/generated/torch.eq.html" target="_blank" rel="noreferrer">torch.eq()</a> <a class="header-anchor" href="#tip4-torch-eq" aria-hidden="true">#</a></h4><p>-&gt; A boolean tensor that is True where input is equal to other and False elsewhere<br> eg👇:</p><blockquote><p>torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))<br> -&gt; tensor([[[True, False], [False, True]]])</p></blockquote><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_subsequence_mask</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">seq</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># 在decoder中用到，屏蔽未来时刻的信息</span></span>
<span class="line"><span style="color:#BABED8;">    attn_shape </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[</span><span style="color:#BABED8;">seq</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">),</span><span style="color:#BABED8;"> seq</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#BABED8;"> seq</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)]</span></span>
<span class="line"><span style="color:#BABED8;">    subsequence_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> np</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">triu</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">np</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ones</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn_shape</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">k</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    subsequence_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_numpy</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">subsequence_mask</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">byte</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># torch.from_numpy().byte() 将numpy数组转换为Tensor</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> subsequence_mask</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h4 id="tip5-np-triu" tabindex="-1">Tip5 <a href="https://numpy.org/doc/stable/reference/generated/numpy.triu.html" target="_blank" rel="noreferrer">np.triu()</a> <a class="header-anchor" href="#tip5-np-triu" aria-hidden="true">#</a></h4><ul><li><strong>np.triu(a, k)</strong> 是取矩阵 a 的上三角数据，但这个三角的斜线位置由 k 的值确定。</li><li><strong>np.tril(a, k)</strong> 是取矩阵 a 的下三角数据</li></ul><h4 id="step3-scaleddotproductattention" tabindex="-1">Step3 ScaledDotProductAttention <a class="header-anchor" href="#step3-scaleddotproductattention" aria-hidden="true">#</a></h4><p>点积注意力公式👇 <img src="`+r+'" alt=""><br><img src="'+c+`" alt=""></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">ScaledDotProductAttention</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">缩放点积注意力 单词间的权重计算</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ScaledDotProductAttention</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        Q: [batch_size, n_heads, len_q, d_k]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        K: [batch_size, n_heads, len_k, d_k]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">Q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">K</span><span style="color:#89DDFF;">:</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Tensor</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">V</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 将Q和K的最后一个维度进行点积，在最后一个维度上进行的。</span></span>
<span class="line"><span style="color:#BABED8;">        scores</span><span style="color:#89DDFF;">:</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Tensor</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            Q</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> K</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">))</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">/</span><span style="color:#BABED8;"> np</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sqrt</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_k</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># mask --- qt~qn =&gt; 很大的负数</span></span>
<span class="line"><span style="color:#BABED8;">        scores</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">masked_fill_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn_mask</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1e9</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># softmax()高得分接近1，低得分接近0，所有概率之和为1</span></span>
<span class="line"><span style="color:#BABED8;">        attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Softmax</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">dim</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)(</span><span style="color:#82AAFF;">scores</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 再乘值向量得到上下文的权重</span></span>
<span class="line"><span style="color:#BABED8;">        context </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">attn</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> V</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> context</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><h4 id="step4-multiheadattention" tabindex="-1">Step4 MultiHeadAttention <a class="header-anchor" href="#step4-multiheadattention" aria-hidden="true">#</a></h4><p>定义多组，让它们分别关注不同的上下文 也增加了可学习的参数 W_Q, W_K, W_V</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">MultiHeadAttention</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">W_Q</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">W_K</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">W_V</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_v </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 将多头注意力的输出进行聚合和转换，将输入维度（batch_size,n_heads*d_v)转换为(~, d_model)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">fc</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_heads </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> d_v</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">input_Q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">input_K</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">input_V</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        input_Q: [batch_size, len_q, d_model]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        input_K: [batch_size, len_k, d_model]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        input_V: [batch_size, len_v(=len_k), d_model]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        attn_mask: [batch_size, seq_len, seq_len]</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 残差</span></span>
<span class="line"><span style="color:#BABED8;">        residual</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> batch_size </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> input_Q</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> input_Q</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        Q </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">W_Q</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_Q</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                                   n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        K </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">W_K</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_K</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                                   n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        V </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">W_V</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_V</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                                   n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_k</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> attn_mask</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">unsqueeze</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">repeat</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_heads</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 实例化-&gt;传递参数</span></span>
<span class="line"><span style="color:#BABED8;">        context</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">ScaledDotProductAttention</span><span style="color:#89DDFF;">()(</span><span style="color:#82AAFF;">Q</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> K</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> V</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> attn_mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        context </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> context</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">reshape</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            batch_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_heads </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> d_v</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 全连接映射成一维矩阵</span></span>
<span class="line"><span style="color:#BABED8;">        output </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">fc</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">context</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 残差</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">LayerNorm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()(</span><span style="color:#82AAFF;">output </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> residual</span><span style="color:#89DDFF;">),</span><span style="color:#BABED8;"> attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><h4 id="tip6-torch-matmul" tabindex="-1">Tip6 <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html" target="_blank" rel="noreferrer">torch.matmul()</a> <a class="header-anchor" href="#tip6-torch-matmul" aria-hidden="true">#</a></h4><p>Matrix product of two tensors 代码示例👇</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">a </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">6</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">reshape</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">b </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">6</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">reshape</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">c </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">a</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> b</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">a</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> b</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> c</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><pre><code>(tensor([[0, 1, 2],
         [3, 4, 5]]),
 tensor([[0, 1],
         [2, 3],
         [4, 5]]),
 tensor([[10, 13],
         [28, 40]]))
</code></pre><h4 id="tip7-masked-fill" tabindex="-1">Tip7 <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" target="_blank" rel="noreferrer">masked_fill()</a> <a class="header-anchor" href="#tip7-masked-fill" aria-hidden="true">#</a></h4><p>Fills elements of self tensor with value where mask is True.<br> The shape of mask must be broadcastable with the shape of the underlying tensor.</p><h4 id="step5-feedforward-layer" tabindex="-1">Step5 FeedForward Layer <a class="header-anchor" href="#step5-feedforward-layer" aria-hidden="true">#</a></h4><p>前馈神经网络<br> 两次线性变换，RELU 作激活层<br> 残差连接（防止原始数据丢失）</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">fc</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Sequential</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_ff</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False),</span></span>
<span class="line"><span style="color:#82AAFF;">            nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ReLU</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">            nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_ff</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">inputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 残差保存原始输入</span></span>
<span class="line"><span style="color:#BABED8;">        residual </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> inputs</span></span>
<span class="line"><span style="color:#BABED8;">        output </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">fc</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">LayerNorm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()(</span><span style="color:#82AAFF;">output </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> residual</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h4 id="step6-encoder" tabindex="-1">Step6 Encoder <a class="header-anchor" href="#step6-encoder" aria-hidden="true">#</a></h4><p>self-attention and feedforward_layer</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">EncoderLayer</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">EncoderLayer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">enc_self_attn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_ffn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_self_attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># K, Q, V, attn_mask</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">enc_self_attn</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_self_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_ffn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">Encoder</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">Encoder Block</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">Encoder</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 词嵌入</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">src_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Embedding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">src_vocab_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 位置编码</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PositionalEncoding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># ? 模块列表，包含多个编码器层</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ModuleList</span><span style="color:#89DDFF;">([</span><span style="color:#82AAFF;">EncoderLayer</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#82AAFF;"> _ </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#82AAFF;"> range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_layers</span><span style="color:#89DDFF;">)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">src_emb</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_emb</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_self_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        enc_self_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[]</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 循环遍历每一个编码器层，将词向量和自注意力掩码传递给每一个层，获取该层的输出及自注意力权重，并存储在列表中</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> layer </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#BABED8;">            enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">layer</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_self_attn_mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">            enc_self_attns</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_self_attn</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attns</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><h4 id="step7-decoder" tabindex="-1">Step7 Decoder <a class="header-anchor" href="#step7-decoder" aria-hidden="true">#</a></h4><p>Masked Multihead attention Multihead attention Feedforward network</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">DecoderLayer</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">DecoderLayer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">dec_self_attn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">dec_enc_attn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">MultiHeadAttention</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_ffn</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PoswiseFeedForwardNet</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_self_attn_mask</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_enc_attn_mask</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dec_self_attn</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_self_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># ?将 dec_outputs 作为生成 Q 的元素，enc_outputs 作为生成 K 和 V 的元素</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dec_enc_attn</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_enc_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_ffn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attn</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attn</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">Decoder</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">Decoder</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">tgt_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Embedding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tgt_vocab_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">pos_emb</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">PositionalEncoding</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ModuleList</span><span style="color:#89DDFF;">([</span><span style="color:#82AAFF;">DecoderLayer</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#82AAFF;"> _ </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#82AAFF;"> range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_layers</span><span style="color:#89DDFF;">)])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_outputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tgt_emb</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">pos_emb</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)).</span><span style="color:#82AAFF;">transpose</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attn_pad_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attn_subsequence_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_subsequence_mask</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># torch.gt(a, value),将 a 中各个位置上的元素和 value 比较，若大于 value，则该位置取 1，否则取 0</span></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">gt</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_self_attn_pad_mask </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> dec_self_attn_subsequence_mask</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">0</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_enc_attn_mask </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">get_attn_pad_mask</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[],</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">[]</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> layer </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#BABED8;">            dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attn</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attn </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">layer</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">                dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_self_attn_mask</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_enc_attn_mask</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">            dec_self_attns</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_self_attn</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">            dec_enc_attns</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_enc_attn</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><h4 id="tip7-torch-gt" tabindex="-1">Tip7 torch.gt() <a class="header-anchor" href="#tip7-torch-gt" aria-hidden="true">#</a></h4><p>Computes input &gt; element-wise<br> torch.gt(a, value) 的意思是，将 a 中各个位置上的元素和 value 比较， 若大于 value，则该位置取 1，否则取 0</p><h4 id="step8-transformer" tabindex="-1">Step8 Transformer <a class="header-anchor" href="#step8-transformer" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#BABED8;"> </span><span style="color:#FFCB6B;">Transformer</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">Transformer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">encoder</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">Encoder</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">decoder</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">Decoder</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">projection</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">d_model</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tgt_vocab_size</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">bias</span><span style="color:#89DDFF;">=False).</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#C792EA;">def</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#BABED8;font-style:italic;">dec_inputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">        enc_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">encoder</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">enc_inputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        dec_outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> s </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">decoder</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> enc_outputs</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        dec_logits </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">projection</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">dec_outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#BABED8;">            dec_logits</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_logits</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)),</span></span>
<span class="line"><span style="color:#BABED8;">            enc_self_attns</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#BABED8;">            dec_self_attns</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#BABED8;">            dec_enc_attns</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><h4 id="step9-损失函数-优化器" tabindex="-1">Step9 损失函数，优化器 <a class="header-anchor" href="#step9-损失函数-优化器" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#BABED8;">model </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">Transformer</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># ignore_index=0,不计算pad的损失</span></span>
<span class="line"><span style="color:#BABED8;">criterion </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">CrossEntropyLoss</span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;font-style:italic;">ignore_index</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">optimizer </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">optim</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">SGD</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">parameters</span><span style="color:#89DDFF;">(),</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">lr</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">1e-3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#BABED8;font-style:italic;">momentum</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0.99</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h4 id="step11-训练" tabindex="-1">Step11 训练 <a class="header-anchor" href="#step11-训练" aria-hidden="true">#</a></h4><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> epoch </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">30</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_outputs </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> loader</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#676E95;font-style:italic;"># 存储到gpu中</span></span>
<span class="line"><span style="color:#BABED8;">        enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#BABED8;">            enc_inputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#BABED8;">            dec_inputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#BABED8;">            dec_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cuda</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        outputs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> enc_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_self_attns</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> dec_enc_attns </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            enc_inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_inputs</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">        loss </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">criterion</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">outputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> dec_outputs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">view</span><span style="color:#89DDFF;">(-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#BABED8;">        </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Epoch:</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#F78C6C;">%04d</span><span style="color:#89DDFF;">&quot;</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">%</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">epoch </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">loss =</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#F78C6C;">{</span><span style="color:#C792EA;">:.6f</span><span style="color:#F78C6C;">}</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">loss</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">        optimizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zero_grad</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        loss</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">backward</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">        optimizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">step</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><hr><p><strong>引用</strong><br><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">Attension is all you need</a><br> 源于<a href="https://www.bilibili.com/video/BV1mk4y1q7eK/?spm_id_from=333.999.0.0&amp;vd_source=744197c073f4828379c29fa20f3ea477" target="_blank" rel="noreferrer">大佬视频讲解</a><br> 本 blog 的<a href="https://github.com/CyanCat22/transfomer_mini_/tree/main/realize" target="_blank" rel="noreferrer">代码链接</a><br><a href="https://github.com/km1994/nlp_paper_study_transformer/tree/main/DL_algorithm/transformer_study/Transformer" target="_blank" rel="noreferrer">杨夕的 Transformer repo</a><br> 分享一个<a href="https://www.zhihu.com/question/556300395" target="_blank" rel="noreferrer">jupyter 转 md</a></p>`,59),D=[y];function i(A,B,u,b,d,m){return n(),a("div",null,D)}const f=s(F,[["render",i]]);export{_ as __pageData,f as default};
