import{_ as s,o as n,c as a,Q as l}from"./chunks/framework.11961b2a.js";const e="/assets/bert_.e4867915.jpg",p="/assets/NSP.41e9038c.png",o="/assets/Difference.8276710b.png",m=JSON.parse('{"title":"Bert 学习笔记 🌟","description":"","frontmatter":{"0":"t","1":"i","2":"t","3":"l","4":"e","5":":","6":"B","7":"e","8":"r","9":"t","10":"_","11":"l","12":"e","13":"a","14":"r","15":"n","16":"_","17":"n","18":"o","19":"t","20":"e","21":"1","22":" ","23":"t","24":"a","25":"g","26":"s","27":":","28":"[","29":"B","30":"e","31":"r","32":"t","33":",","34":" ","35":"n","36":"l","37":"p","38":"]"},"headers":[{"level":2,"title":"语言模型 LM 的好处","slug":"语言模型-lm-的好处","link":"#语言模型-lm-的好处","children":[]},{"level":2,"title":"迁移学习","slug":"迁移学习","link":"#迁移学习","children":[]},{"level":2,"title":"Bert 预训练任务一：Masked Language Model","slug":"bert-预训练任务一-masked-language-model","link":"#bert-预训练任务一-masked-language-model","children":[{"level":3,"title":"Tokens","slug":"tokens","link":"#tokens","children":[]},{"level":3,"title":"实现细节","slug":"实现细节","link":"#实现细节","children":[]}]},{"level":2,"title":"Bert 预训练任务二：Next Sentence Prediction","slug":"bert-预训练任务二-next-sentence-prediction","link":"#bert-预训练任务二-next-sentence-prediction","children":[]},{"level":2,"title":"ELMo, Bert, Gpt","slug":"elmo-bert-gpt","link":"#elmo-bert-gpt","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"relativePath":"src/Bert/Bert.md","lastUpdated":1702353660000}'),t={name:"src/Bert/Bert.md"},r=l('<h1 id="bert-学习笔记-🌟" tabindex="-1">Bert 学习笔记 🌟 <a class="header-anchor" href="#bert-学习笔记-🌟" aria-hidden="true">#</a></h1><p>全称：<strong>Bidirectinal Encoder Representation from Transformer</strong><br> 是 Google 以无监督的方式利用大量<strong>无标注</strong>的文本练成的<strong>语言模型</strong></p><p>Google 在训练 Bert 时让他同时进行两个预训练任务：</p><ol><li>完形填空 Masked Language Model</li><li>判断第 2 个句子在原始文本中与第 1 个是否相接 Next Sentence Prediction</li></ol><p><img src="'+e+`" alt=""></p><h2 id="语言模型-lm-的好处" tabindex="-1">语言模型 LM 的好处 <a class="header-anchor" href="#语言模型-lm-的好处" aria-hidden="true">#</a></h2><ul><li>无监督数据无限大，不用找人来标注数据</li><li>能顾学习语法结构、解读语义和指代消解（Coreference Resolution），能更有效的训练下游任务并提高其表现</li><li>减少处理不同 NLP 任务所需 architecture engineering 成本（人力、时间、计算资源）</li></ul><h2 id="迁移学习" tabindex="-1">迁移学习 <a class="header-anchor" href="#迁移学习" aria-hidden="true">#</a></h2><ul><li>先以 LM Pretraining 的方式预先训练出一个对自然语言有一定「理解」的通用模型</li><li>再将该模型拿來做特征提取或是 fine tune 下游的（监督式）任务</li></ul><h2 id="bert-预训练任务一-masked-language-model" tabindex="-1">Bert 预训练任务一：Masked Language Model <a class="header-anchor" href="#bert-预训练任务一-masked-language-model" aria-hidden="true">#</a></h2><p>先用 transfomers 中的 bert 预训练模型感受一下哈~</p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#BABED8;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> BertTokenizer</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#BABED8;"> IPython</span><span style="color:#89DDFF;">.</span><span style="color:#BABED8;">display </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> clear_output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">使用中文 BERT-BASE 预训练模型 Use Bert!</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#BABED8;">PRETRAINED_MODEL_NAME </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">bert-base-chinese</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 取得此预训练模型所使用的 tokenizer</span></span>
<span class="line"><span style="color:#BABED8;">tokenizer </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> BertTokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PRETRAINED_MODEL_NAME</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#82AAFF;">clear_output</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#BABED8;">vocab </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">vocab</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">字典大小</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> len</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">vocab</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#BABED8;">text </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">[CLS] 等到潮水 [MASK] 了，就知道谁没穿裤子。</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#BABED8;">tokens </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tokenize</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">text</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">ids </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">convert_tokens_to_ids</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">text</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ids</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;[MASK]&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">载入已经训练好的 masked 语言模型并对有 [MASK] 的句子做预测</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#BABED8;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#BABED8;"> BertForMaskedLM</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 除了 tokens 以外我們还需要辨別句子的 segment ids</span></span>
<span class="line"><span style="color:#BABED8;">tokens_tensor </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tensor</span><span style="color:#89DDFF;">([</span><span style="color:#82AAFF;">ids</span><span style="color:#89DDFF;">])</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># (1, seq_len)</span></span>
<span class="line"><span style="color:#BABED8;">segments_tensors </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros_like</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens_tensor</span><span style="color:#89DDFF;">)</span><span style="color:#BABED8;">  </span><span style="color:#676E95;font-style:italic;"># (1, seq_len)</span></span>
<span class="line"><span style="color:#BABED8;">maskedLM_model </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> BertForMaskedLM</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">PRETRAINED_MODEL_NAME</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">clear_output</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 使用 masked LM 估计 [MASK] 位置所代表的实际 token</span></span>
<span class="line"><span style="color:#BABED8;">maskedLM_model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">eval</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">with</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">no_grad</span><span style="color:#89DDFF;">():</span></span>
<span class="line"><span style="color:#BABED8;">    outputs </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">maskedLM_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokens_tensor</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> segments_tensors</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">    predictions </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> outputs</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#676E95;font-style:italic;"># (1, seq_len, num_hidden_units)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">del</span><span style="color:#BABED8;"> maskedLM_model</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 将 [MASK] 位置的分布取 top k 最有可能的 tokens 出來</span></span>
<span class="line"><span style="color:#BABED8;">masked_index </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">5</span></span>
<span class="line"><span style="color:#BABED8;">k </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> </span><span style="color:#F78C6C;">3</span></span>
<span class="line"><span style="color:#BABED8;">probs</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> indices </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">topk</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">softmax</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">predictions</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> masked_index</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">-</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> k</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#BABED8;">predicted_tokens </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">convert_ids_to_tokens</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">indices</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">tolist</span><span style="color:#89DDFF;">())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 我们取 top 1 当做预测值</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">输入 tokens ：</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokens</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">-</span><span style="color:#89DDFF;">&quot;</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">50</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#BABED8;"> i</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">(</span><span style="color:#BABED8;">t</span><span style="color:#89DDFF;">,</span><span style="color:#BABED8;"> p</span><span style="color:#89DDFF;">)</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#BABED8;"> </span><span style="color:#82AAFF;">enumerate</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">zip</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">predicted_tokens</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> probs</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#BABED8;">    tokens</span><span style="color:#89DDFF;">[</span><span style="color:#BABED8;">masked_index</span><span style="color:#89DDFF;">]</span><span style="color:#BABED8;"> </span><span style="color:#89DDFF;">=</span><span style="color:#BABED8;"> t</span></span>
<span class="line"><span style="color:#BABED8;">    </span><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Top </span><span style="color:#F78C6C;">{}</span><span style="color:#C3E88D;"> (</span><span style="color:#F78C6C;">{</span><span style="color:#C792EA;">:2</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">%)：</span><span style="color:#F78C6C;">{}</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        i</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#FFCB6B;">int</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">p</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">item</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> tokens</span><span style="color:#89DDFF;">[:</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">]),</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">...</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">輸入 tokens ： [&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;[MASK]&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">--------------------------------------------------</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Top 1 (82%)：[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;來&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Top 2 (11%)：[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;濕&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Top 3 ( 2%)：[&#39;[CLS]&#39;, &#39;等&#39;, &#39;到&#39;, &#39;潮&#39;, &#39;水&#39;, &#39;過&#39;, &#39;了&#39;, &#39;，&#39;, &#39;就&#39;, &#39;知&#39;] ...</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="tokens" tabindex="-1">Tokens <a class="header-anchor" href="#tokens" aria-hidden="true">#</a></h3><p><code>[CLS]</code> 输入序列的 repr., 一般放在句子开头<br><code>[SEP]</code> 分隔符 两个句子的交界，在两个句子之间或是句子末尾<br><code>[UNK]</code> 未出现的词汇 (unknown)<br><code>[PAD]</code> zero_padding 将长度不一样的序列补齐方便做 batch 运算<br><code>[MASK]</code> 未知屏蔽，仅在预训练阶段会用到</p><h3 id="实现细节" tabindex="-1">实现细节 <a class="header-anchor" href="#实现细节" aria-hidden="true">#</a></h3><p>随机遮盖或替换一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后做 Loss 的时候也只计算被遮盖部分的 Loss</p><ol><li><p>随机把一句话中 15% 的 token（字或词）替换成以下内容：<br> a. 这些 token 有 80% 的几率被替换成 [MASK]，例如 my dog is hairy→my dog is [MASK]<br> b. 有 10% 的几率被替换成任意一个其它的 token，例如 my dog is hairy→my dog is apple<br> c. 有 10% 的几率原封不动，例如 my dog is hairy→my dog is hairy</p></li><li><p>之后让模型预测和还原被遮盖掉或替换掉的部分，计算损失的时候，只计算在第 1 步里被随机遮盖或替换的部分，其余部分不做损失，其余部分无论输出什么东西，都无所谓</p></li></ol><p>这样做的好处是，BERT 并不知道 [MASK] 替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。 这样可以强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行 &quot;纠错&quot;。比如在上面的例子中，模型在编码 apple 时，根据上下文 my dog is，应该把 apple 编码成 hairy 的语义而不是 apple 的语义</p><h2 id="bert-预训练任务二-next-sentence-prediction" tabindex="-1">Bert 预训练任务二：Next Sentence Prediction <a class="header-anchor" href="#bert-预训练任务二-next-sentence-prediction" aria-hidden="true">#</a></h2><blockquote><p>LML(完型填空)能让 Bert 更好的 model 每个词汇在不同语境下的 repr.，而 NSP 任务则能帮助 Bert model 两个句子之间的关系，这在问答系统 QA、自然语言推论 NLI 任务有很大帮助</p></blockquote><p><img src="`+p+'" alt=""></p><ul><li>Token Embedding 就是正常的词向量，即 nn.Embedding()</li><li>Segment Embedding 作用是用 embedding 的信息让模型分开上下句，给上句的 token 全 0，下句的 token 全 1，让模型得以判断上下句的起止位置<br> [CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]<br> 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1</li><li>Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的</li></ul><p>BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加</p><h2 id="elmo-bert-gpt" tabindex="-1">ELMo, Bert, Gpt <a class="header-anchor" href="#elmo-bert-gpt" aria-hidden="true">#</a></h2><p><img src="'+o+'" alt=""></p><ul><li><p><a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noreferrer">ELMo</a>(Embedding from Languang model) 利用独立训练的<strong>双向两层 LSTM</strong> 做语言模型并将中间得到的隐状态向量串接当作每个词汇的 contextual word repr.</p></li><li><p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noreferrer">GPT</a>(Generative Pre-Training) 则是使用 <strong>Transformer 的 Decoder</strong> 来训练一个中规中矩，从左到右的单向语言模型，使用的数据、参数量庞大</p></li><li><p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noreferrer">BERT</a> 跟它们的差异在于利用<strong>MLM（克漏字）<strong>的概念及</strong>Transformer Encoder</strong>的架构，摆脱以往语言模型只能从单个方向（由左到右或由右到左）估计下个词汇的出现几率，训练出一个双向的语言代表模型。 这使得 BERT 输出的每个 token 的 repr. 都同时蕴含了前后文信息，实现了真正的双向 representation</p></li></ul><hr><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-hidden="true">#</a></h2><p><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" target="_blank" rel="noreferrer">leemeng Bert 巨人之力</a><br><a href="https://wmathor.com/index.php/archives/1456/" target="_blank" rel="noreferrer">Bert note</a><br><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html" target="_blank" rel="noreferrer">GPT 生成金庸小说</a><br><a href="https://github.com/leemengtw/deep-learning-resources" target="_blank" rel="noreferrer">深度学习资源 repo </a><br><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html" target="_blank" rel="noreferrer">李宏毅 ML 课程</a><br><a href="https://github.com/km1994/nlp_paper_study_transformer/tree/main/DL_algorithm/transformer_study/Transformer" target="_blank" rel="noreferrer">杨夕大佬的 repo</a></p>',31),c=[r];function i(F,y,D,d,b,A){return n(),a("div",null,c)}const B=s(t,[["render",i]]);export{m as __pageData,B as default};
